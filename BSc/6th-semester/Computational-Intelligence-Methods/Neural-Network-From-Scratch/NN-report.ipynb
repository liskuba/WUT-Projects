{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size=None, output_size=None, weights=None, bias=None):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        if (input_size is None or output_size is None) and (weights is None or bias is None):\n",
    "            raise Exception('input and output sizes or weights with bias are required')\n",
    "        if weights is None:\n",
    "            self.weights = np.random.rand(input_size, output_size) # uniform\n",
    "        else:\n",
    "            self.weights = weights\n",
    "            self.input_size = len(self.weights)\n",
    "            self.output_size = len(self.weights[0])\n",
    "        if bias is None:\n",
    "            self.bias = np.random.rand(1, output_size)\n",
    "        else:\n",
    "            self.bias = bias\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        d_weights = -learning_rate * weights_error\n",
    "        d_bias = -learning_rate * output_error\n",
    "        return input_error, d_weights, d_bias\n",
    "    \n",
    "    def update_weights(self, d_weights, d_bias):\n",
    "        self.weights += d_weights\n",
    "        self.bias += d_bias\n",
    "        \n",
    "    \n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d234c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_list(l, new_el):\n",
    "    new_l = [None] * len(l)\n",
    "    for i in range(len(l)-1, 0, -1):\n",
    "        new_l[i] = l[i-1]\n",
    "    new_l[0] = new_el\n",
    "    return new_l\n",
    "\n",
    "def check_early_stopping(l):\n",
    "    for i in range(len(l)-1):\n",
    "        if l[i] < l[i+1]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informational-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "        self.errors = []\n",
    "        self.val_errors = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate, batch_size, x_val=None,\n",
    "            y_val=None, early_stopping=None, regularisation=None):\n",
    "        \n",
    "        \n",
    "        if regularisation is not None:\n",
    "            if type(regularisation) != list:\n",
    "                raise Exception(\"Regularisation should be list, example: ['L1', 0.1], where first\\\n",
    "                                element is type of regularisation, second is parameter (lambda)\")\n",
    "\n",
    "        samples = len(x_train)\n",
    "        \n",
    "        if early_stopping != None:\n",
    "            if x_val is None or y_val is None:\n",
    "                raise Exception('Missing validations sets')\n",
    "            last_errors = [i for i in range(early_stopping)]\n",
    "        \n",
    "        batch_proportion = batch_size/samples\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            \n",
    "            for batch_number in range(int(1/batch_proportion)):\n",
    "                \n",
    "                layersDeltaWeight = []\n",
    "                layersDeltaBias = []\n",
    "                \n",
    "                for j in range(int(batch_number * batch_size), int((batch_number+1)*batch_size)):\n",
    "                    \n",
    "                    #forward propagation\n",
    "                    \n",
    "                    output = x_train[j]\n",
    "                    \n",
    "                    for layer in self.layers:\n",
    "                        output = layer.forward_propagation(output)\n",
    "\n",
    "                    err += self.loss(y_train[j], output)\n",
    "                    \n",
    "                    if regularisation is not None:\n",
    "                        \n",
    "                        full_layers_weights = []\n",
    "                        for layer in self.layers:\n",
    "                            if isinstance(layer, Layer):\n",
    "                                full_layers_weights.append(layer.weights)\n",
    "                        \n",
    "                        err_reg = 0\n",
    "                                \n",
    "                        if regularisation[0] == 'L1':\n",
    "                  \n",
    "                            for weights in full_layers_weights:\n",
    "                                err_reg += np.sum(np.abs(weights))\n",
    "                                \n",
    "                        elif regularisation[0] == 'L2':\n",
    "                            \n",
    "                            for weights in full_layers_weights:\n",
    "                                err_reg += np.sum(np.power(weights, 2))\n",
    "                            \n",
    "                        else:\n",
    "                            raise Exception('Incorrect regularisation type')\n",
    "                        \n",
    "                        err_reg *= regularisation[1] / (2*samples)\n",
    "                        err += err_reg\n",
    "                    \n",
    "                    #backward propagation\n",
    "                    \n",
    "                    \n",
    "                    error = self.loss_prime(y_train[j], output)\n",
    "                    \n",
    "                    \n",
    "                    tmplayerDeltaWeights = []\n",
    "                    tmplayerDeltaBias = []\n",
    "                    for layer in reversed(self.layers):\n",
    "                        \n",
    "                        if isinstance(layer, ActivationLayer):\n",
    "                            \n",
    "                            error = layer.backward_propagation(error, learning_rate)\n",
    "                            \n",
    "                        else:\n",
    "                            error, deltaWeights, deltaBias = layer.backward_propagation(error, learning_rate)\n",
    "                            tmplayerDeltaWeights.append(deltaWeights)\n",
    "                            tmplayerDeltaBias.append(deltaBias)\n",
    "                        \n",
    "                    layersDeltaWeight.append(tmplayerDeltaWeights)\n",
    "                    layersDeltaBias.append(tmplayerDeltaBias)\n",
    "                    \n",
    "                for layerdWeight, layerdBias in zip(layersDeltaWeight, layersDeltaBias):\n",
    "                    \n",
    "                    only_full_layers = []\n",
    "                    \n",
    "                    for layer in reversed(self.layers):\n",
    "                        \n",
    "                        if isinstance(layer, Layer):\n",
    "                            \n",
    "                            only_full_layers.append(layer)\n",
    "                            \n",
    "                    for dWeight, dBias, layer in zip(layerdWeight, layerdBias, only_full_layers):\n",
    "                        \n",
    "                        error_reg = 0\n",
    "                        \n",
    "                        if regularisation is not None:             \n",
    "                            if regularisation[0] == 'L2':\n",
    "                                \n",
    "                                error_reg += layer.weights * regularisation[1] / samples\n",
    "                                \n",
    "                        layer.update_weights(dWeight + error_reg, dBias)\n",
    "\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            \n",
    "            self.errors.append(err)\n",
    "            \n",
    "            if early_stopping != None:\n",
    "                y_predicted_val = self.predict(x_val)\n",
    "                new_err = self.loss(y_val, y_predicted_val)\n",
    "\n",
    "                last_errors = update_list(last_errors, new_err)\n",
    "                self.val_errors.append(new_err)\n",
    "\n",
    "                if check_early_stopping(last_errors):\n",
    "                    print(f'Last errors on validation set: {last_errors}')\n",
    "                    print('early stopped')\n",
    "                    break\n",
    "                    \n",
    "            if early_stopping is None and x_val is not None:\n",
    "                \n",
    "                y_predicted_val = self.predict(x_val)\n",
    "                new_err = self.loss(y_val, y_predicted_val)\n",
    "                self.val_errors.append(new_err)\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "    def fit_momentum(self, x_train, y_train, epochs, learning_rate, batch_size, lambd):\n",
    "\n",
    "        samples = len(x_train)\n",
    "        \n",
    "        momentum_w = []\n",
    "        momentum_b = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            if isinstance(layer, ActivationLayer):\n",
    "                continue  \n",
    "                \n",
    "            momentum_w.append(deepcopy(layer.weights))\n",
    "            momentum_b.append(deepcopy(layer.bias))\n",
    "        \n",
    "        momentum_w = np.array(momentum_w, dtype=object)\n",
    "        \n",
    "        for i in range(len(momentum_w)):\n",
    "            momentum_w[i][:] = 0\n",
    "            momentum_b[i][:] = 0\n",
    "\n",
    "        batch_proportion = batch_size/samples\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            \n",
    "            for batch_number in range(int(1/batch_proportion)):\n",
    "                \n",
    "                layersDeltaWeight = []\n",
    "                layersDeltaBias = []\n",
    "                \n",
    "                for j in range(int(batch_number * batch_size), int((batch_number+1)*batch_size)):\n",
    "                    \n",
    "                    #forward propagation\n",
    "                    \n",
    "                    output = x_train[j]\n",
    "                    \n",
    "                    for layer in self.layers:\n",
    "                        output = layer.forward_propagation(output)\n",
    "\n",
    "                    err += self.loss(y_train[j], output)\n",
    "                    \n",
    "                    #backward propagation\n",
    "                    \n",
    "                    error = self.loss_prime(y_train[j], output)\n",
    "                    \n",
    "                    tmplayerDeltaWeights = []\n",
    "                    tmplayerDeltaBias = []\n",
    "                    for layer in reversed(self.layers):\n",
    "                        \n",
    "                        if isinstance(layer, ActivationLayer):\n",
    "                            \n",
    "                            error = layer.backward_propagation(error, learning_rate)\n",
    "                            \n",
    "                        else:\n",
    "                            \n",
    "                            error, deltaWeights, deltaBias = layer.backward_propagation(error, learning_rate)\n",
    "                            tmplayerDeltaWeights.append(deltaWeights)\n",
    "                            tmplayerDeltaBias.append(deltaBias)\n",
    "                        \n",
    "                    layersDeltaWeight.append(tmplayerDeltaWeights)\n",
    "                    layersDeltaBias.append(tmplayerDeltaBias)\n",
    "                    \n",
    "                for layerdWeight, layerdBias in zip(layersDeltaWeight, layersDeltaBias):\n",
    "                    \n",
    "                    only_full_layers = []\n",
    "                    \n",
    "                    for layer in reversed(self.layers):\n",
    "                        \n",
    "                        if isinstance(layer, Layer):\n",
    "                            \n",
    "                            only_full_layers.append(layer)\n",
    "                            \n",
    "                    l = len(only_full_layers)\n",
    "                            \n",
    "                    for dWeight, dBias, layer in zip(layerdWeight, layerdBias, only_full_layers):\n",
    "                        \n",
    "                        l -= 1\n",
    "                        momentum_w[l] = dWeight + learning_rate * momentum_w[l] * lambd\n",
    "                        momentum_b[l] = dBias + learning_rate * momentum_b[l] * lambd\n",
    "                        layer.update_weights(momentum_w[l], momentum_b[l])\n",
    "\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            self.errors.append(err)\n",
    "            \n",
    "    def fit_RMSProp(self, x_train, y_train, epochs, learning_rate, batch_size, beta):\n",
    "        \n",
    "        samples = len(x_train)\n",
    "        \n",
    "        E_w = []\n",
    "        E_b = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            if isinstance(layer, ActivationLayer):\n",
    "                continue  \n",
    "                \n",
    "            E_w.append(deepcopy(layer.weights))\n",
    "            E_b.append(deepcopy(layer.bias))\n",
    "        \n",
    "        E_w = np.array(E_w, dtype=object)\n",
    "        \n",
    "        for i in range(len(E_w)):\n",
    "            E_w[i][:] = 0\n",
    "            E_b[i][:] = 0\n",
    "            \n",
    "        batch_proportion = batch_size/samples\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            \n",
    "            for batch_number in range(int(1/batch_proportion)):\n",
    "                \n",
    "                layersDeltaWeight = []\n",
    "                layersDeltaBias = []\n",
    "                \n",
    "                for j in range(int(batch_number * batch_size), int((batch_number+1)*batch_size)):\n",
    "                    \n",
    "                    #forward propagation\n",
    "                    \n",
    "                    output = x_train[j]\n",
    "                    \n",
    "                    for layer in self.layers:\n",
    "                        output = layer.forward_propagation(output)\n",
    "\n",
    "                    err += self.loss(y_train[j], output)\n",
    "                    \n",
    "                    #backward propagation\n",
    "                    \n",
    "                    error = self.loss_prime(y_train[j], output)\n",
    "                    \n",
    "                    tmplayerDeltaWeights = []\n",
    "                    tmplayerDeltaBias = []\n",
    "                    for layer in reversed(self.layers):\n",
    "                        \n",
    "                        if isinstance(layer, ActivationLayer):\n",
    "                            \n",
    "                            error = layer.backward_propagation(error, learning_rate)\n",
    "                            \n",
    "                        else:\n",
    "                            \n",
    "                            error, deltaWeights, deltaBias = layer.backward_propagation(error, learning_rate)\n",
    "                            tmplayerDeltaWeights.append(deltaWeights)\n",
    "                            tmplayerDeltaBias.append(deltaBias)\n",
    "                        \n",
    "                    layersDeltaWeight.append(tmplayerDeltaWeights)\n",
    "                    layersDeltaBias.append(tmplayerDeltaBias)\n",
    "                    \n",
    "                for layerdWeight, layerdBias in zip(layersDeltaWeight, layersDeltaBias):\n",
    "                    \n",
    "                    only_full_layers = []\n",
    "                    \n",
    "                    for layer in reversed(self.layers):\n",
    "                        \n",
    "                        if isinstance(layer, Layer):\n",
    "                            \n",
    "                            only_full_layers.append(layer)\n",
    "                            \n",
    "                    l = len(only_full_layers)\n",
    "                            \n",
    "                    for dWeight, dBias, layer in zip(layerdWeight, layerdBias, only_full_layers):\n",
    "                        \n",
    "                        l -= 1\n",
    "                        E_w[l] = beta * E_w[l] + (1 - beta) * dWeight ** 2\n",
    "                        E_b[l] = beta * E_b[l] + (1 - beta) * dBias ** 2\n",
    "                        layer.update_weights(learning_rate * dWeight / np.sqrt(E_w[l]),\n",
    "                                             learning_rate * dBias / np.sqrt(E_b[l]))\n",
    "\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            self.errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alive-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    e = np.exp((-1)*x)\n",
    "    return 1/(1+e)\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    e = np.exp((-1)*x)\n",
    "    return e / ((1+e)**2)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def derivative_linear(x):\n",
    "    return np.array([1 for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53718c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / np.sum(expX)\n",
    "\n",
    "def derivative_softmax(x):\n",
    "    s = softmax(x)[0]\n",
    "    jacobian_m = np.diag(s)\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m[0])):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = 1 - s[i] ** 2\n",
    "            else:\n",
    "                jacobian_m[i][j] = -1 * s[i] * s[j]\n",
    "    return np.diag(jacobian_m)\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return 1-tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hybrid-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7339ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_true, y_pred):  # cross-entropy\n",
    "    \n",
    "    y = [[0 for i in range(len(net.layers[len(net.layers)-2].output[0]))]]\n",
    "    y[0][y_true[0][0]] = 1\n",
    "    y = np.array(y)\n",
    "\n",
    "    return -np.mean(y * np.log(y_pred + 1e-8))\n",
    "\n",
    "def cost_function_prime(y_true, y_pred):\n",
    "    \n",
    "    y = [[0 for i in range(len(net.layers[len(net.layers)-2].output[0]))]]\n",
    "    y[0][y_true[0][0]] = 1\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b9d80",
   "metadata": {},
   "source": [
    "# NN2 PROPAGACJA WSTECZNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ee7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-simple-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08759b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2203)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.0001, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35075a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "\n",
    "data = pd.read_csv('mio1/regression/square-simple-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "out = net.predict(x_test)\n",
    "mse1 = mse(y_test, out)\n",
    "\n",
    "print(mse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ff3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/steps-small-training.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "np.random.seed(244)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_test, y_test, epochs=1000, learning_rate=0.005, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "err2 = net.errors\n",
    "\n",
    "data = pd.read_csv('mio1/regression/steps-small-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "out = net.predict(x_test)\n",
    "mse2 = mse(y_test, out)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5331fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/multimodal-large-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 32))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(32, 32))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(32, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=100, learning_rate=0.0005, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f07285",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/multimodal-large-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "err3 = net.errors\n",
    "\n",
    "\n",
    "out = net.predict(x_test)\n",
    "mse3 = mse(y_test, out)\n",
    "print(mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7964fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, 1001)), err1, label='square-simple')\n",
    "plt.plot(list(range(1, 1001)), err2, label='steps-small')\n",
    "plt.plot(list(range(1, 101)), err3, label='multimodal-large')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb123a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-simple-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fce1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2203)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.0001, batch_size=1)\n",
    "\n",
    "err1 = net.errors\n",
    "\n",
    "data = pd.read_csv('mio1/regression/square-simple-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "out = net.predict(x_test)\n",
    "mse1 = mse(y_test, out)\n",
    "print(mse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f725b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2203)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.0001, batch_size=10)\n",
    "\n",
    "err2 = net.errors\n",
    "\n",
    "data = pd.read_csv('mio1/regression/square-simple-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "out = net.predict(x_test)\n",
    "mse2 = mse(y_test, out)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91777a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2203)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.00006, batch_size=100)\n",
    "\n",
    "err3 = net.errors\n",
    "\n",
    "data = pd.read_csv('mio1/regression/square-simple-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "out = net.predict(x_test)\n",
    "mse3 = mse(y_test, out)\n",
    "print(mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ced441",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2203)\n",
    "\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.000095, batch_size=50)\n",
    "\n",
    "err4 = net.errors\n",
    "\n",
    "data = pd.read_csv('mio1/regression/square-simple-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[[x]] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "out = net.predict(x_test)\n",
    "mse4 = mse(y_test, out)\n",
    "print(mse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, 1001)), err1, label='batch_size=1')\n",
    "plt.plot(list(range(1, 1001)), err2, label='batch_size=10')\n",
    "plt.plot(list(range(1, 1001)), err4, label='batch_size=50')\n",
    "plt.plot(list(range(1, 1001)), err3, label='batch_size=100')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN2_batch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7094b9",
   "metadata": {},
   "source": [
    "# NN3 MOMENTUM I RMSPROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756be1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_momentum(x_train, y_train, epochs=100, learning_rate=0.0004, batch_size=100, lambd=0.5)\n",
    "\n",
    "err1 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ace152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_momentum(x_train, y_train, epochs=100, learning_rate=0.0002, batch_size=100, lambd=0.5)\n",
    "\n",
    "err2 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac95496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_momentum(x_train, y_train, epochs=100, learning_rate=0.0001, batch_size=100, lambd=0.5)\n",
    "\n",
    "err3 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d239c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_momentum(x_train, y_train, epochs=100, learning_rate=0.00008, batch_size=100, lambd=0.5)\n",
    "\n",
    "err4 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e2f47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-training.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_momentum(x_train, y_train, epochs=100, learning_rate=0.00001, batch_size=100, lambd=0.5)\n",
    "\n",
    "err5 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, 101)), err2, label='learning_rate=0.0004')\n",
    "plt.plot(list(range(1, 101)), err1, label='learning_rate=0.0002')\n",
    "plt.plot(list(range(1, 101)), err3, label='learning_rate=0.0001')\n",
    "plt.plot(list(range(1, 101)), err4, label='learning_rate=0.00008')\n",
    "plt.plot(list(range(1, 101)), err5, label='learning_rate=0.00001')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN3_momentum.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a55bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.01, batch_size=100, beta=0.9)\n",
    "\n",
    "err1 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91dfaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.005, batch_size=100, beta=0.9)\n",
    "\n",
    "err2 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bf626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.001, batch_size=100, beta=0.9)\n",
    "\n",
    "err3 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaae954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.0008, batch_size=100, beta=0.9)\n",
    "\n",
    "err4 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a01bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.0004, batch_size=100, beta=0.9)\n",
    "\n",
    "err5 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beaae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, 501)), err1, label='learning_rate=0.01')\n",
    "plt.plot(list(range(1, 501)), err2, label='learning_rate=0.005')\n",
    "plt.plot(list(range(1, 501)), err3, label='learning_rate=0.001')\n",
    "plt.plot(list(range(1, 501)), err4, label='learning_rate=0.0008')\n",
    "plt.plot(list(range(1, 501)), err5, label='learning_rate=0.0004')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN3_rmsprop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea831b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.005, batch_size=100, beta=0.9)\n",
    "\n",
    "err1 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f855358",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.005, batch_size=100, beta=0.8)\n",
    "\n",
    "err2 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.005, batch_size=100, beta=0.7)\n",
    "\n",
    "err3 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7019e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/square-large-test.csv')\n",
    "x_train = list(data.x)\n",
    "y_train = list(data.y)\n",
    "x_train = np.array([[[x]] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "\n",
    "np.random.seed(2203)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit_RMSProp(x_train, y_train, epochs=500, learning_rate=0.005, batch_size=100, beta=0.3)\n",
    "\n",
    "err4 = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, 501)), err1, label='beta=0.9')\n",
    "plt.plot(list(range(1, 501)), err2, label='beta=0.8')\n",
    "plt.plot(list(range(1, 501)), err3, label='beta=0.7')\n",
    "plt.plot(list(range(1, 501)), err4, label='beta=0.3')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN3_beta.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de760b35",
   "metadata": {},
   "source": [
    "# NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/classification/easy-training.csv')\n",
    "y_train = list(data.c.astype(int))\n",
    "x_train = data[['x', 'y']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "seaborn.relplot(data=data, x='x', y='y', hue='c', aspect=1.61)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[x] for x in x_train])\n",
    "y_train = np.array([[[y]] for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401750da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2203) # 2203\n",
    "net = Network()\n",
    "net.add(Layer(2, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 2))\n",
    "net.add(ActivationLayer(softmax, derivative_softmax))\n",
    "\n",
    "net.use(cost_function, cost_function_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.0001, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = net.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f6969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, 1001)), err)\n",
    "\n",
    "plt.ylabel('cross-entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN4_easy_learning.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bce0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/classification/easy-test.csv')\n",
    "y_test = list(data.c.astype(int))\n",
    "x_test = data[['x', 'y']].values.tolist()\n",
    "x_test = np.array([[x] for x in x_test])\n",
    "out = net.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ee818",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [np.argmax(l) for l in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48994133",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'x': data.x, 'y': data.y, 'c': out}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,2)\n",
    "fig.set_size_inches(14, 7)\n",
    "\n",
    "seaborn.scatterplot(data=data, x='x', y='y', hue='c', ax=ax[0])\n",
    "seaborn.scatterplot(data=df, x='x', y='y', hue='c', ax=ax[1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063f5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "798497db",
   "metadata": {},
   "source": [
    "# NN5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e82475",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/multimodal-large-training.csv')\n",
    "\n",
    "y_train = list(data.y)\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "x_train = list(data.x)\n",
    "x_train = np.array([[x] for x in x_train])\n",
    "\n",
    "data = pd.read_csv('mio1/regression/multimodal-large-test.csv')\n",
    "\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[x] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c0b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 50))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.00005, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fea103",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse1 = mse(out, y_test)\n",
    "print(mse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81153f2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(45)\n",
    "net = Network()\n",
    "net.add(Layer(1, 50))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0001, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "err2 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse2 = mse(out, y_test)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(45)\n",
    "net = Network()\n",
    "net.add(Layer(1, 50))\n",
    "net.add(ActivationLayer(tanh, derivative_tanh))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0002, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "err3 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse3 = mse(out, y_test)\n",
    "print(mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db36e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "net = Network()\n",
    "net.add(Layer(1, 200))\n",
    "net.add(ActivationLayer(relu, derivative_tanh))\n",
    "net.add(Layer(200, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.000008, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "err4 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse4 = mse(out, y_test)\n",
    "print(mse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, 201))\n",
    "\n",
    "plt.plot(x, err1, label='sigmoid')\n",
    "plt.plot(x, err2, label='linear')\n",
    "plt.plot(x, err3, label='tanh')\n",
    "plt.plot(x, err4, label='ReLU')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49fba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(15)\n",
    "net = Network()\n",
    "net.add(Layer(1, 20))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(20, 30))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(30, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.00013, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse1 = mse(out, y_test)\n",
    "print(mse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaad824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(45)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "net.add(Layer(100, 50))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.00000001, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "err2 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse2 = mse(out, y_test)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "net = Network()\n",
    "net.add(Layer(1, 10))\n",
    "net.add(ActivationLayer(tanh, derivative_tanh))\n",
    "net.add(Layer(10, 40))\n",
    "net.add(ActivationLayer(tanh, derivative_tanh))\n",
    "net.add(Layer(40, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0000999, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "err3 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse3 = mse(out, y_test)\n",
    "print(mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc131e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(Layer(1, 20))\n",
    "net.add(ActivationLayer(relu, derivative_relu))\n",
    "net.add(Layer(20, 10))\n",
    "net.add(ActivationLayer(relu, derivative_relu))\n",
    "net.add(Layer(10, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.00001, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a759f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "err4 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse4 = mse(out, y_test)\n",
    "print(mse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, 201))\n",
    "\n",
    "plt.plot(x, err1, label='sigmoid')\n",
    "plt.plot(x, err2, label='linear')\n",
    "plt.plot(x, err3, label='tanh')\n",
    "plt.plot(x, err4, label='ReLU')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0, 7000])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN5_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(Layer(1, 20))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(20, 20))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(20, 30))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(30, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0007, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse1 = mse(out, y_test)\n",
    "print(mse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4258781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(Layer(1, 10))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "net.add(Layer(10, 10))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "net.add(Layer(10, 10))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "net.add(Layer(10, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.00001, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "err2 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse2 = mse(out, y_test)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(Layer(1, 5))\n",
    "net.add(ActivationLayer(tanh, derivative_tanh))\n",
    "net.add(Layer(5, 10))\n",
    "net.add(ActivationLayer(tanh, derivative_tanh))\n",
    "net.add(Layer(10, 50))\n",
    "net.add(ActivationLayer(tanh, derivative_tanh))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.00005, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4eb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "err3 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse3 = mse(out, y_test)\n",
    "print(mse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "net = Network()\n",
    "net.add(Layer(1, 32))\n",
    "net.add(ActivationLayer(relu, derivative_relu))\n",
    "net.add(Layer(32, 16))\n",
    "net.add(ActivationLayer(relu, derivative_relu))\n",
    "net.add(Layer(16, 8))\n",
    "net.add(ActivationLayer(relu, derivative_relu))\n",
    "net.add(Layer(8, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.000001, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "err4 = net.errors\n",
    "out = net.predict(x_test)\n",
    "mse4 = mse(out, y_test)\n",
    "print(mse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, 201))\n",
    "\n",
    "plt.plot(x, err1, label='sigmoid')\n",
    "plt.plot(x, err2, label='linear')\n",
    "plt.plot(x, err3, label='tanh')\n",
    "plt.plot(x, err4, label='ReLU')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('NN5_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d567631",
   "metadata": {},
   "source": [
    "# NN6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mio1/regression/multimodal-large-training.csv')\n",
    "y_train = list(data.y)\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "x_train = list(data.x)\n",
    "x_train = np.array([[x] for x in x_train])\n",
    "\n",
    "data = pd.read_csv('mio1/regression/multimodal-large-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[x] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state=420, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc86c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 50))\n",
    "net.add(ActivationLayer(tanh, derivative_sigmoid))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0005, batch_size=50,\n",
    "        early_stopping=10, x_val=x_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "err2 = net.val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a1f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 50))\n",
    "net.add(ActivationLayer(tanh, derivative_sigmoid))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0005, batch_size=50, x_val=x_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a36cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "err3 = net.errors\n",
    "err4 = net.val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23527dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(err1)+1))\n",
    "\n",
    "plt.plot(x, err1, label='train')\n",
    "plt.plot(x, err2, label='val')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(err3)+1))\n",
    "\n",
    "plt.plot(x, err3, label='train')\n",
    "plt.plot(x, err4, label='val')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c7741b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('mio1/regression/multimodal-sparse-training.csv')\n",
    "y_train = list(data.y)\n",
    "y_train = np.array([[[y]] for y in y_train])\n",
    "x_train = list(data.x)\n",
    "x_train = np.array([[x] for x in x_train])\n",
    "\n",
    "data = pd.read_csv('mio1/regression/multimodal-sparse-test.csv')\n",
    "x_test = list(data.x)\n",
    "y_test = list(data.y)\n",
    "x_test = np.array([[x] for x in x_test])\n",
    "y_test = np.array([[[y]] for y in y_test])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state=420, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ae0abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2000   error=5269.591878\n",
      "epoch 2/2000   error=5146.413828\n",
      "epoch 3/2000   error=5136.758342\n",
      "epoch 4/2000   error=5128.400794\n",
      "epoch 5/2000   error=5119.289423\n",
      "epoch 6/2000   error=5109.247760\n",
      "epoch 7/2000   error=5098.031250\n",
      "epoch 8/2000   error=5085.358883\n",
      "epoch 9/2000   error=5070.928355\n",
      "epoch 10/2000   error=5054.424503\n",
      "epoch 11/2000   error=5035.531947\n",
      "epoch 12/2000   error=5013.953722\n",
      "epoch 13/2000   error=4989.434629\n",
      "epoch 14/2000   error=4961.786230\n",
      "epoch 15/2000   error=4930.909295\n",
      "epoch 16/2000   error=4896.809523\n",
      "epoch 17/2000   error=4859.603508\n",
      "epoch 18/2000   error=4819.513924\n",
      "epoch 19/2000   error=4776.854917\n",
      "epoch 20/2000   error=4732.010179\n",
      "epoch 21/2000   error=4685.406822\n",
      "epoch 22/2000   error=4637.488146\n",
      "epoch 23/2000   error=4588.687994\n",
      "epoch 24/2000   error=4539.408796\n",
      "epoch 25/2000   error=4490.004756\n",
      "epoch 26/2000   error=4440.770944\n",
      "epoch 27/2000   error=4391.938350\n",
      "epoch 28/2000   error=4343.674361\n",
      "epoch 29/2000   error=4296.087628\n",
      "epoch 30/2000   error=4249.236036\n",
      "epoch 31/2000   error=4203.136303\n",
      "epoch 32/2000   error=4157.773807\n",
      "epoch 33/2000   error=4113.111516\n",
      "epoch 34/2000   error=4069.097420\n",
      "epoch 35/2000   error=4025.670367\n",
      "epoch 36/2000   error=3982.764512\n",
      "epoch 37/2000   error=3940.312666\n",
      "epoch 38/2000   error=3898.248781\n",
      "epoch 39/2000   error=3856.509711\n",
      "epoch 40/2000   error=3815.036417\n",
      "epoch 41/2000   error=3773.774702\n",
      "epoch 42/2000   error=3732.675658\n",
      "epoch 43/2000   error=3691.695965\n",
      "epoch 44/2000   error=3650.798251\n",
      "epoch 45/2000   error=3609.951689\n",
      "epoch 46/2000   error=3569.132953\n",
      "epoch 47/2000   error=3528.327519\n",
      "epoch 48/2000   error=3487.531189\n",
      "epoch 49/2000   error=3446.751566\n",
      "epoch 50/2000   error=3406.009185\n",
      "epoch 51/2000   error=3365.337973\n",
      "epoch 52/2000   error=3324.784834\n",
      "epoch 53/2000   error=3284.408257\n",
      "epoch 54/2000   error=3244.276042\n",
      "epoch 55/2000   error=3204.462346\n",
      "epoch 56/2000   error=3165.044393\n",
      "epoch 57/2000   error=3126.099203\n",
      "epoch 58/2000   error=3087.700660\n",
      "epoch 59/2000   error=3049.917170\n",
      "epoch 60/2000   error=3012.810021\n",
      "epoch 61/2000   error=2976.432452\n",
      "epoch 62/2000   error=2940.829352\n",
      "epoch 63/2000   error=2906.037449\n",
      "epoch 64/2000   error=2872.085839\n",
      "epoch 65/2000   error=2838.996715\n",
      "epoch 66/2000   error=2806.786186\n",
      "epoch 67/2000   error=2775.465109\n",
      "epoch 68/2000   error=2745.039880\n",
      "epoch 69/2000   error=2715.513150\n",
      "epoch 70/2000   error=2686.884448\n",
      "epoch 71/2000   error=2659.150710\n",
      "epoch 72/2000   error=2632.306706\n",
      "epoch 73/2000   error=2606.345364\n",
      "epoch 74/2000   error=2581.258008\n",
      "epoch 75/2000   error=2557.034509\n",
      "epoch 76/2000   error=2533.663370\n",
      "epoch 77/2000   error=2511.131760\n",
      "epoch 78/2000   error=2489.425513\n",
      "epoch 79/2000   error=2468.529113\n",
      "epoch 80/2000   error=2448.425676\n",
      "epoch 81/2000   error=2429.096953\n",
      "epoch 82/2000   error=2410.523347\n",
      "epoch 83/2000   error=2392.683972\n",
      "epoch 84/2000   error=2375.556737\n",
      "epoch 85/2000   error=2359.118459\n",
      "epoch 86/2000   error=2343.345010\n",
      "epoch 87/2000   error=2328.211483\n",
      "epoch 88/2000   error=2313.692369\n",
      "epoch 89/2000   error=2299.761748\n",
      "epoch 90/2000   error=2286.393478\n",
      "epoch 91/2000   error=2273.561385\n",
      "epoch 92/2000   error=2261.239431\n",
      "epoch 93/2000   error=2249.401885\n",
      "epoch 94/2000   error=2238.023467\n",
      "epoch 95/2000   error=2227.079479\n",
      "epoch 96/2000   error=2216.545912\n",
      "epoch 97/2000   error=2206.399542\n",
      "epoch 98/2000   error=2196.617998\n",
      "epoch 99/2000   error=2187.179823\n",
      "epoch 100/2000   error=2178.064509\n",
      "epoch 101/2000   error=2169.252521\n",
      "epoch 102/2000   error=2160.725310\n",
      "epoch 103/2000   error=2152.465314\n",
      "epoch 104/2000   error=2144.455944\n",
      "epoch 105/2000   error=2136.681568\n",
      "epoch 106/2000   error=2129.127486\n",
      "epoch 107/2000   error=2121.779902\n",
      "epoch 108/2000   error=2114.625883\n",
      "epoch 109/2000   error=2107.653330\n",
      "epoch 110/2000   error=2100.850935\n",
      "epoch 111/2000   error=2094.208138\n",
      "epoch 112/2000   error=2087.715092\n",
      "epoch 113/2000   error=2081.362618\n",
      "epoch 114/2000   error=2075.142165\n",
      "epoch 115/2000   error=2069.045775\n",
      "epoch 116/2000   error=2063.066038\n",
      "epoch 117/2000   error=2057.196061\n",
      "epoch 118/2000   error=2051.429430\n",
      "epoch 119/2000   error=2045.760178\n",
      "epoch 120/2000   error=2040.182752\n",
      "epoch 121/2000   error=2034.691986\n",
      "epoch 122/2000   error=2029.283068\n",
      "epoch 123/2000   error=2023.951517\n",
      "epoch 124/2000   error=2018.693158\n",
      "epoch 125/2000   error=2013.504099\n",
      "epoch 126/2000   error=2008.380707\n",
      "epoch 127/2000   error=2003.319590\n",
      "epoch 128/2000   error=1998.317581\n",
      "epoch 129/2000   error=1993.371713\n",
      "epoch 130/2000   error=1988.479210\n",
      "epoch 131/2000   error=1983.637470\n",
      "epoch 132/2000   error=1978.844049\n",
      "epoch 133/2000   error=1974.096651\n",
      "epoch 134/2000   error=1969.393114\n",
      "epoch 135/2000   error=1964.731402\n",
      "epoch 136/2000   error=1960.109590\n",
      "epoch 137/2000   error=1955.525860\n",
      "epoch 138/2000   error=1950.978489\n",
      "epoch 139/2000   error=1946.465840\n",
      "epoch 140/2000   error=1941.986359\n",
      "epoch 141/2000   error=1937.538567\n",
      "epoch 142/2000   error=1933.121049\n",
      "epoch 143/2000   error=1928.732457\n",
      "epoch 144/2000   error=1924.371499\n",
      "epoch 145/2000   error=1920.036935\n",
      "epoch 146/2000   error=1915.727577\n",
      "epoch 147/2000   error=1911.442281\n",
      "epoch 148/2000   error=1907.179947\n",
      "epoch 149/2000   error=1902.939517\n",
      "epoch 150/2000   error=1898.719968\n",
      "epoch 151/2000   error=1894.520315\n",
      "epoch 152/2000   error=1890.339610\n",
      "epoch 153/2000   error=1886.176934\n",
      "epoch 154/2000   error=1882.031404\n",
      "epoch 155/2000   error=1877.902168\n",
      "epoch 156/2000   error=1873.788401\n",
      "epoch 157/2000   error=1869.689312\n",
      "epoch 158/2000   error=1865.604137\n",
      "epoch 159/2000   error=1861.532141\n",
      "epoch 160/2000   error=1857.472617\n",
      "epoch 161/2000   error=1853.424886\n",
      "epoch 162/2000   error=1849.388299\n",
      "epoch 163/2000   error=1845.362230\n",
      "epoch 164/2000   error=1841.346084\n",
      "epoch 165/2000   error=1837.339294\n",
      "epoch 166/2000   error=1833.341318\n",
      "epoch 167/2000   error=1829.351647\n",
      "epoch 168/2000   error=1825.369799\n",
      "epoch 169/2000   error=1821.395321\n",
      "epoch 170/2000   error=1817.427795\n",
      "epoch 171/2000   error=1813.466830\n",
      "epoch 172/2000   error=1809.512073\n",
      "epoch 173/2000   error=1805.563201\n",
      "epoch 174/2000   error=1801.619930\n",
      "epoch 175/2000   error=1797.682009\n",
      "epoch 176/2000   error=1793.749223\n",
      "epoch 177/2000   error=1789.821395\n",
      "epoch 178/2000   error=1785.898383\n",
      "epoch 179/2000   error=1781.980081\n",
      "epoch 180/2000   error=1778.066417\n",
      "epoch 181/2000   error=1774.157351\n",
      "epoch 182/2000   error=1770.252877\n",
      "epoch 183/2000   error=1766.353013\n",
      "epoch 184/2000   error=1762.457807\n",
      "epoch 185/2000   error=1758.567326\n",
      "epoch 186/2000   error=1754.681659\n",
      "epoch 187/2000   error=1750.800910\n",
      "epoch 188/2000   error=1746.925195\n",
      "epoch 189/2000   error=1743.054638\n",
      "epoch 190/2000   error=1739.189372\n",
      "epoch 191/2000   error=1735.329530\n",
      "epoch 192/2000   error=1731.475245\n",
      "epoch 193/2000   error=1727.626649\n",
      "epoch 194/2000   error=1723.783869\n",
      "epoch 195/2000   error=1719.947025\n",
      "epoch 196/2000   error=1716.116231\n",
      "epoch 197/2000   error=1712.291590\n",
      "epoch 198/2000   error=1708.473199\n",
      "epoch 199/2000   error=1704.661140\n",
      "epoch 200/2000   error=1700.855488\n",
      "epoch 201/2000   error=1697.056306\n",
      "epoch 202/2000   error=1693.263646\n",
      "epoch 203/2000   error=1689.477549\n",
      "epoch 204/2000   error=1685.698044\n",
      "epoch 205/2000   error=1681.925151\n",
      "epoch 206/2000   error=1678.158877\n",
      "epoch 207/2000   error=1674.399222\n",
      "epoch 208/2000   error=1670.646174\n",
      "epoch 209/2000   error=1666.899714\n",
      "epoch 210/2000   error=1663.159811\n",
      "epoch 211/2000   error=1659.426432\n",
      "epoch 212/2000   error=1655.699533\n",
      "epoch 213/2000   error=1651.979066\n",
      "epoch 214/2000   error=1648.264980\n",
      "epoch 215/2000   error=1644.557221\n",
      "epoch 216/2000   error=1640.855733\n",
      "epoch 217/2000   error=1637.160461\n",
      "epoch 218/2000   error=1633.471353\n",
      "epoch 219/2000   error=1629.788361\n",
      "epoch 220/2000   error=1626.111444\n",
      "epoch 221/2000   error=1622.440567\n",
      "epoch 222/2000   error=1618.775708\n",
      "epoch 223/2000   error=1615.116856\n",
      "epoch 224/2000   error=1611.464012\n",
      "epoch 225/2000   error=1607.817193\n",
      "epoch 226/2000   error=1604.176432\n",
      "epoch 227/2000   error=1600.541777\n",
      "epoch 228/2000   error=1596.913291\n",
      "epoch 229/2000   error=1593.291056\n",
      "epoch 230/2000   error=1589.675163\n",
      "epoch 231/2000   error=1586.065718\n",
      "epoch 232/2000   error=1582.462839\n",
      "epoch 233/2000   error=1578.866649\n",
      "epoch 234/2000   error=1575.277278\n",
      "epoch 235/2000   error=1571.694859\n",
      "epoch 236/2000   error=1568.119525\n",
      "epoch 237/2000   error=1564.551406\n",
      "epoch 238/2000   error=1560.990632\n",
      "epoch 239/2000   error=1557.437325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 240/2000   error=1553.891601\n",
      "epoch 241/2000   error=1550.353571\n",
      "epoch 242/2000   error=1546.823339\n",
      "epoch 243/2000   error=1543.301003\n",
      "epoch 244/2000   error=1539.786655\n",
      "epoch 245/2000   error=1536.280383\n",
      "epoch 246/2000   error=1532.782270\n",
      "epoch 247/2000   error=1529.292400\n",
      "epoch 248/2000   error=1525.810851\n",
      "epoch 249/2000   error=1522.337705\n",
      "epoch 250/2000   error=1518.873042\n",
      "epoch 251/2000   error=1515.416944\n",
      "epoch 252/2000   error=1511.969497\n",
      "epoch 253/2000   error=1508.530789\n",
      "epoch 254/2000   error=1505.100911\n",
      "epoch 255/2000   error=1501.679959\n",
      "epoch 256/2000   error=1498.268033\n",
      "epoch 257/2000   error=1494.865238\n",
      "epoch 258/2000   error=1491.471682\n",
      "epoch 259/2000   error=1488.087481\n",
      "epoch 260/2000   error=1484.712751\n",
      "epoch 261/2000   error=1481.347617\n",
      "epoch 262/2000   error=1477.992204\n",
      "epoch 263/2000   error=1474.646644\n",
      "epoch 264/2000   error=1471.311072\n",
      "epoch 265/2000   error=1467.985626\n",
      "epoch 266/2000   error=1464.670448\n",
      "epoch 267/2000   error=1461.365682\n",
      "epoch 268/2000   error=1458.071475\n",
      "epoch 269/2000   error=1454.787976\n",
      "epoch 270/2000   error=1451.515336\n",
      "epoch 271/2000   error=1448.253707\n",
      "epoch 272/2000   error=1445.003245\n",
      "epoch 273/2000   error=1441.764103\n",
      "epoch 274/2000   error=1438.536437\n",
      "epoch 275/2000   error=1435.320402\n",
      "epoch 276/2000   error=1432.116154\n",
      "epoch 277/2000   error=1428.923849\n",
      "epoch 278/2000   error=1425.743641\n",
      "epoch 279/2000   error=1422.575685\n",
      "epoch 280/2000   error=1419.420132\n",
      "epoch 281/2000   error=1416.277135\n",
      "epoch 282/2000   error=1413.146843\n",
      "epoch 283/2000   error=1410.029404\n",
      "epoch 284/2000   error=1406.924964\n",
      "epoch 285/2000   error=1403.833667\n",
      "epoch 286/2000   error=1400.755654\n",
      "epoch 287/2000   error=1397.691064\n",
      "epoch 288/2000   error=1394.640031\n",
      "epoch 289/2000   error=1391.602688\n",
      "epoch 290/2000   error=1388.579164\n",
      "epoch 291/2000   error=1385.569585\n",
      "epoch 292/2000   error=1382.574072\n",
      "epoch 293/2000   error=1379.592744\n",
      "epoch 294/2000   error=1376.625715\n",
      "epoch 295/2000   error=1373.673093\n",
      "epoch 296/2000   error=1370.734986\n",
      "epoch 297/2000   error=1367.811493\n",
      "epoch 298/2000   error=1364.902711\n",
      "epoch 299/2000   error=1362.008732\n",
      "epoch 300/2000   error=1359.129642\n",
      "epoch 301/2000   error=1356.265524\n",
      "epoch 302/2000   error=1353.416452\n",
      "epoch 303/2000   error=1350.582499\n",
      "epoch 304/2000   error=1347.763731\n",
      "epoch 305/2000   error=1344.960207\n",
      "epoch 306/2000   error=1342.171982\n",
      "epoch 307/2000   error=1339.399106\n",
      "epoch 308/2000   error=1336.641623\n",
      "epoch 309/2000   error=1333.899570\n",
      "epoch 310/2000   error=1331.172979\n",
      "epoch 311/2000   error=1328.461878\n",
      "epoch 312/2000   error=1325.766287\n",
      "epoch 313/2000   error=1323.086222\n",
      "epoch 314/2000   error=1320.421693\n",
      "epoch 315/2000   error=1317.772706\n",
      "epoch 316/2000   error=1315.139260\n",
      "epoch 317/2000   error=1312.521350\n",
      "epoch 318/2000   error=1309.918966\n",
      "epoch 319/2000   error=1307.332094\n",
      "epoch 320/2000   error=1304.760714\n",
      "epoch 321/2000   error=1302.204803\n",
      "epoch 322/2000   error=1299.664335\n",
      "epoch 323/2000   error=1297.139277\n",
      "epoch 324/2000   error=1294.629596\n",
      "epoch 325/2000   error=1292.135253\n",
      "epoch 326/2000   error=1289.656206\n",
      "epoch 327/2000   error=1287.192410\n",
      "epoch 328/2000   error=1284.743817\n",
      "epoch 329/2000   error=1282.310375\n",
      "epoch 330/2000   error=1279.892029\n",
      "epoch 331/2000   error=1277.488720\n",
      "epoch 332/2000   error=1275.100385\n",
      "epoch 333/2000   error=1272.726958\n",
      "epoch 334/2000   error=1270.368370\n",
      "epoch 335/2000   error=1268.024543\n",
      "epoch 336/2000   error=1265.695400\n",
      "epoch 337/2000   error=1263.380855\n",
      "epoch 338/2000   error=1261.080817\n",
      "epoch 339/2000   error=1258.795191\n",
      "epoch 340/2000   error=1256.523875\n",
      "epoch 341/2000   error=1254.266760\n",
      "epoch 342/2000   error=1252.023731\n",
      "epoch 343/2000   error=1249.794665\n",
      "epoch 344/2000   error=1247.579433\n",
      "epoch 345/2000   error=1245.377898\n",
      "epoch 346/2000   error=1243.189915\n",
      "epoch 347/2000   error=1241.015329\n",
      "epoch 348/2000   error=1238.853980\n",
      "epoch 349/2000   error=1236.705697\n",
      "epoch 350/2000   error=1234.570301\n",
      "epoch 351/2000   error=1232.447604\n",
      "epoch 352/2000   error=1230.337408\n",
      "epoch 353/2000   error=1228.239507\n",
      "epoch 354/2000   error=1226.153684\n",
      "epoch 355/2000   error=1224.079713\n",
      "epoch 356/2000   error=1222.017358\n",
      "epoch 357/2000   error=1219.966372\n",
      "epoch 358/2000   error=1217.926500\n",
      "epoch 359/2000   error=1215.897475\n",
      "epoch 360/2000   error=1213.879021\n",
      "epoch 361/2000   error=1211.870853\n",
      "epoch 362/2000   error=1209.872674\n",
      "epoch 363/2000   error=1207.884181\n",
      "epoch 364/2000   error=1205.905062\n",
      "epoch 365/2000   error=1203.934994\n",
      "epoch 366/2000   error=1201.973652\n",
      "epoch 367/2000   error=1200.020700\n",
      "epoch 368/2000   error=1198.075801\n",
      "epoch 369/2000   error=1196.138612\n",
      "epoch 370/2000   error=1194.208788\n",
      "epoch 371/2000   error=1192.285981\n",
      "epoch 372/2000   error=1190.369845\n",
      "epoch 373/2000   error=1188.460034\n",
      "epoch 374/2000   error=1186.556204\n",
      "epoch 375/2000   error=1184.658014\n",
      "epoch 376/2000   error=1182.765127\n",
      "epoch 377/2000   error=1180.877210\n",
      "epoch 378/2000   error=1178.993934\n",
      "epoch 379/2000   error=1177.114976\n",
      "epoch 380/2000   error=1175.240016\n",
      "epoch 381/2000   error=1173.368738\n",
      "epoch 382/2000   error=1171.500829\n",
      "epoch 383/2000   error=1169.635978\n",
      "epoch 384/2000   error=1167.773876\n",
      "epoch 385/2000   error=1165.914210\n",
      "epoch 386/2000   error=1164.056669\n",
      "epoch 387/2000   error=1162.200935\n",
      "epoch 388/2000   error=1160.346687\n",
      "epoch 389/2000   error=1158.493599\n",
      "epoch 390/2000   error=1156.641334\n",
      "epoch 391/2000   error=1154.789549\n",
      "epoch 392/2000   error=1152.937890\n",
      "epoch 393/2000   error=1151.085996\n",
      "epoch 394/2000   error=1149.233491\n",
      "epoch 395/2000   error=1147.379993\n",
      "epoch 396/2000   error=1145.525108\n",
      "epoch 397/2000   error=1143.668432\n",
      "epoch 398/2000   error=1141.809555\n",
      "epoch 399/2000   error=1139.948059\n",
      "epoch 400/2000   error=1138.083521\n",
      "epoch 401/2000   error=1136.215514\n",
      "epoch 402/2000   error=1134.343610\n",
      "epoch 403/2000   error=1132.467384\n",
      "epoch 404/2000   error=1130.586414\n",
      "epoch 405/2000   error=1128.700282\n",
      "epoch 406/2000   error=1126.808582\n",
      "epoch 407/2000   error=1124.910916\n",
      "epoch 408/2000   error=1123.006901\n",
      "epoch 409/2000   error=1121.096168\n",
      "epoch 410/2000   error=1119.178363\n",
      "epoch 411/2000   error=1117.253151\n",
      "epoch 412/2000   error=1115.320214\n",
      "epoch 413/2000   error=1113.379255\n",
      "epoch 414/2000   error=1111.429992\n",
      "epoch 415/2000   error=1109.472163\n",
      "epoch 416/2000   error=1107.505524\n",
      "epoch 417/2000   error=1105.529847\n",
      "epoch 418/2000   error=1103.544920\n",
      "epoch 419/2000   error=1101.550543\n",
      "epoch 420/2000   error=1099.546533\n",
      "epoch 421/2000   error=1097.532715\n",
      "epoch 422/2000   error=1095.508927\n",
      "epoch 423/2000   error=1093.475016\n",
      "epoch 424/2000   error=1091.430835\n",
      "epoch 425/2000   error=1089.376247\n",
      "epoch 426/2000   error=1087.311120\n",
      "epoch 427/2000   error=1085.235329\n",
      "epoch 428/2000   error=1083.148754\n",
      "epoch 429/2000   error=1081.051280\n",
      "epoch 430/2000   error=1078.942798\n",
      "epoch 431/2000   error=1076.823202\n",
      "epoch 432/2000   error=1074.692395\n",
      "epoch 433/2000   error=1072.550284\n",
      "epoch 434/2000   error=1070.396779\n",
      "epoch 435/2000   error=1068.231801\n",
      "epoch 436/2000   error=1066.055274\n",
      "epoch 437/2000   error=1063.867130\n",
      "epoch 438/2000   error=1061.667309\n",
      "epoch 439/2000   error=1059.455757\n",
      "epoch 440/2000   error=1057.232429\n",
      "epoch 441/2000   error=1054.997288\n",
      "epoch 442/2000   error=1052.750305\n",
      "epoch 443/2000   error=1050.491461\n",
      "epoch 444/2000   error=1048.220743\n",
      "epoch 445/2000   error=1045.938149\n",
      "epoch 446/2000   error=1043.643686\n",
      "epoch 447/2000   error=1041.337368\n",
      "epoch 448/2000   error=1039.019220\n",
      "epoch 449/2000   error=1036.689276\n",
      "epoch 450/2000   error=1034.347576\n",
      "epoch 451/2000   error=1031.994172\n",
      "epoch 452/2000   error=1029.629123\n",
      "epoch 453/2000   error=1027.252498\n",
      "epoch 454/2000   error=1024.864372\n",
      "epoch 455/2000   error=1022.464829\n",
      "epoch 456/2000   error=1020.053964\n",
      "epoch 457/2000   error=1017.631875\n",
      "epoch 458/2000   error=1015.198672\n",
      "epoch 459/2000   error=1012.754469\n",
      "epoch 460/2000   error=1010.299389\n",
      "epoch 461/2000   error=1007.833563\n",
      "epoch 462/2000   error=1005.357125\n",
      "epoch 463/2000   error=1002.870220\n",
      "epoch 464/2000   error=1000.372997\n",
      "epoch 465/2000   error=997.865611\n",
      "epoch 466/2000   error=995.348222\n",
      "epoch 467/2000   error=992.820998\n",
      "epoch 468/2000   error=990.284110\n",
      "epoch 469/2000   error=987.737734\n",
      "epoch 470/2000   error=985.182053\n",
      "epoch 471/2000   error=982.617251\n",
      "epoch 472/2000   error=980.043518\n",
      "epoch 473/2000   error=977.461049\n",
      "epoch 474/2000   error=974.870040\n",
      "epoch 475/2000   error=972.270692\n",
      "epoch 476/2000   error=969.663208\n",
      "epoch 477/2000   error=967.047796\n",
      "epoch 478/2000   error=964.424663\n",
      "epoch 479/2000   error=961.794021\n",
      "epoch 480/2000   error=959.156084\n",
      "epoch 481/2000   error=956.511065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 482/2000   error=953.859182\n",
      "epoch 483/2000   error=951.200652\n",
      "epoch 484/2000   error=948.535693\n",
      "epoch 485/2000   error=945.864525\n",
      "epoch 486/2000   error=943.187368\n",
      "epoch 487/2000   error=940.504442\n",
      "epoch 488/2000   error=937.815969\n",
      "epoch 489/2000   error=935.122168\n",
      "epoch 490/2000   error=932.423260\n",
      "epoch 491/2000   error=929.719466\n",
      "epoch 492/2000   error=927.011004\n",
      "epoch 493/2000   error=924.298094\n",
      "epoch 494/2000   error=921.580954\n",
      "epoch 495/2000   error=918.859801\n",
      "epoch 496/2000   error=916.134851\n",
      "epoch 497/2000   error=913.406320\n",
      "epoch 498/2000   error=910.674422\n",
      "epoch 499/2000   error=907.939368\n",
      "epoch 500/2000   error=905.201371\n",
      "epoch 501/2000   error=902.460640\n",
      "epoch 502/2000   error=899.717383\n",
      "epoch 503/2000   error=896.971807\n",
      "epoch 504/2000   error=894.224116\n",
      "epoch 505/2000   error=891.474515\n",
      "epoch 506/2000   error=888.723205\n",
      "epoch 507/2000   error=885.970385\n",
      "epoch 508/2000   error=883.216254\n",
      "epoch 509/2000   error=880.461008\n",
      "epoch 510/2000   error=877.704842\n",
      "epoch 511/2000   error=874.947948\n",
      "epoch 512/2000   error=872.190518\n",
      "epoch 513/2000   error=869.432741\n",
      "epoch 514/2000   error=866.674803\n",
      "epoch 515/2000   error=863.916891\n",
      "epoch 516/2000   error=861.159188\n",
      "epoch 517/2000   error=858.401876\n",
      "epoch 518/2000   error=855.645135\n",
      "epoch 519/2000   error=852.889144\n",
      "epoch 520/2000   error=850.134078\n",
      "epoch 521/2000   error=847.380112\n",
      "epoch 522/2000   error=844.627419\n",
      "epoch 523/2000   error=841.876170\n",
      "epoch 524/2000   error=839.126534\n",
      "epoch 525/2000   error=836.378678\n",
      "epoch 526/2000   error=833.632768\n",
      "epoch 527/2000   error=830.888967\n",
      "epoch 528/2000   error=828.147438\n",
      "epoch 529/2000   error=825.408338\n",
      "epoch 530/2000   error=822.671828\n",
      "epoch 531/2000   error=819.938063\n",
      "epoch 532/2000   error=817.207197\n",
      "epoch 533/2000   error=814.479382\n",
      "epoch 534/2000   error=811.754770\n",
      "epoch 535/2000   error=809.033508\n",
      "epoch 536/2000   error=806.315743\n",
      "epoch 537/2000   error=803.601619\n",
      "epoch 538/2000   error=800.891279\n",
      "epoch 539/2000   error=798.184864\n",
      "epoch 540/2000   error=795.482512\n",
      "epoch 541/2000   error=792.784359\n",
      "epoch 542/2000   error=790.090541\n",
      "epoch 543/2000   error=787.401188\n",
      "epoch 544/2000   error=784.716431\n",
      "epoch 545/2000   error=782.036398\n",
      "epoch 546/2000   error=779.361215\n",
      "epoch 547/2000   error=776.691006\n",
      "epoch 548/2000   error=774.025892\n",
      "epoch 549/2000   error=771.365992\n",
      "epoch 550/2000   error=768.711423\n",
      "epoch 551/2000   error=766.062300\n",
      "epoch 552/2000   error=763.418735\n",
      "epoch 553/2000   error=760.780839\n",
      "epoch 554/2000   error=758.148719\n",
      "epoch 555/2000   error=755.522482\n",
      "epoch 556/2000   error=752.902230\n",
      "epoch 557/2000   error=750.288065\n",
      "epoch 558/2000   error=747.680085\n",
      "epoch 559/2000   error=745.078388\n",
      "epoch 560/2000   error=742.483066\n",
      "epoch 561/2000   error=739.894213\n",
      "epoch 562/2000   error=737.311919\n",
      "epoch 563/2000   error=734.736270\n",
      "epoch 564/2000   error=732.167352\n",
      "epoch 565/2000   error=729.605248\n",
      "epoch 566/2000   error=727.050040\n",
      "epoch 567/2000   error=724.501807\n",
      "epoch 568/2000   error=721.960624\n",
      "epoch 569/2000   error=719.426567\n",
      "epoch 570/2000   error=716.899709\n",
      "epoch 571/2000   error=714.380121\n",
      "epoch 572/2000   error=711.867870\n",
      "epoch 573/2000   error=709.363024\n",
      "epoch 574/2000   error=706.865649\n",
      "epoch 575/2000   error=704.375806\n",
      "epoch 576/2000   error=701.893558\n",
      "epoch 577/2000   error=699.418963\n",
      "epoch 578/2000   error=696.952080\n",
      "epoch 579/2000   error=694.492965\n",
      "epoch 580/2000   error=692.041672\n",
      "epoch 581/2000   error=689.598254\n",
      "epoch 582/2000   error=687.162762\n",
      "epoch 583/2000   error=684.735246\n",
      "epoch 584/2000   error=682.315753\n",
      "epoch 585/2000   error=679.904332\n",
      "epoch 586/2000   error=677.501026\n",
      "epoch 587/2000   error=675.105881\n",
      "epoch 588/2000   error=672.718937\n",
      "epoch 589/2000   error=670.340236\n",
      "epoch 590/2000   error=667.969817\n",
      "epoch 591/2000   error=665.607720\n",
      "epoch 592/2000   error=663.253980\n",
      "epoch 593/2000   error=660.908634\n",
      "epoch 594/2000   error=658.571715\n",
      "epoch 595/2000   error=656.243257\n",
      "epoch 596/2000   error=653.923292\n",
      "epoch 597/2000   error=651.611850\n",
      "epoch 598/2000   error=649.308960\n",
      "epoch 599/2000   error=647.014651\n",
      "epoch 600/2000   error=644.728949\n",
      "epoch 601/2000   error=642.451880\n",
      "epoch 602/2000   error=640.183468\n",
      "epoch 603/2000   error=637.923736\n",
      "epoch 604/2000   error=635.672707\n",
      "epoch 605/2000   error=633.430401\n",
      "epoch 606/2000   error=631.196838\n",
      "epoch 607/2000   error=628.972036\n",
      "epoch 608/2000   error=626.756012\n",
      "epoch 609/2000   error=624.548784\n",
      "epoch 610/2000   error=622.350364\n",
      "epoch 611/2000   error=620.160769\n",
      "epoch 612/2000   error=617.980010\n",
      "epoch 613/2000   error=615.808098\n",
      "epoch 614/2000   error=613.645046\n",
      "epoch 615/2000   error=611.490862\n",
      "epoch 616/2000   error=609.345554\n",
      "epoch 617/2000   error=607.209131\n",
      "epoch 618/2000   error=605.081598\n",
      "epoch 619/2000   error=602.962962\n",
      "epoch 620/2000   error=600.853226\n",
      "epoch 621/2000   error=598.752395\n",
      "epoch 622/2000   error=596.660471\n",
      "epoch 623/2000   error=594.577455\n",
      "epoch 624/2000   error=592.503349\n",
      "epoch 625/2000   error=590.438153\n",
      "epoch 626/2000   error=588.381866\n",
      "epoch 627/2000   error=586.334486\n",
      "epoch 628/2000   error=584.296011\n",
      "epoch 629/2000   error=582.266439\n",
      "epoch 630/2000   error=580.245764\n",
      "epoch 631/2000   error=578.233982\n",
      "epoch 632/2000   error=576.231089\n",
      "epoch 633/2000   error=574.237079\n",
      "epoch 634/2000   error=572.251944\n",
      "epoch 635/2000   error=570.275677\n",
      "epoch 636/2000   error=568.308272\n",
      "epoch 637/2000   error=566.349719\n",
      "epoch 638/2000   error=564.400010\n",
      "epoch 639/2000   error=562.459135\n",
      "epoch 640/2000   error=560.527085\n",
      "epoch 641/2000   error=558.603849\n",
      "epoch 642/2000   error=556.689415\n",
      "epoch 643/2000   error=554.783774\n",
      "epoch 644/2000   error=552.886913\n",
      "epoch 645/2000   error=550.998819\n",
      "epoch 646/2000   error=549.119480\n",
      "epoch 647/2000   error=547.248884\n",
      "epoch 648/2000   error=545.387016\n",
      "epoch 649/2000   error=543.533864\n",
      "epoch 650/2000   error=541.689412\n",
      "epoch 651/2000   error=539.853647\n",
      "epoch 652/2000   error=538.026554\n",
      "epoch 653/2000   error=536.208117\n",
      "epoch 654/2000   error=534.398322\n",
      "epoch 655/2000   error=532.597152\n",
      "epoch 656/2000   error=530.804591\n",
      "epoch 657/2000   error=529.020624\n",
      "epoch 658/2000   error=527.245233\n",
      "epoch 659/2000   error=525.478402\n",
      "epoch 660/2000   error=523.720114\n",
      "epoch 661/2000   error=521.970351\n",
      "epoch 662/2000   error=520.229096\n",
      "epoch 663/2000   error=518.496330\n",
      "epoch 664/2000   error=516.772037\n",
      "epoch 665/2000   error=515.056196\n",
      "epoch 666/2000   error=513.348791\n",
      "epoch 667/2000   error=511.649801\n",
      "epoch 668/2000   error=509.959208\n",
      "epoch 669/2000   error=508.276993\n",
      "epoch 670/2000   error=506.603135\n",
      "epoch 671/2000   error=504.937616\n",
      "epoch 672/2000   error=503.280416\n",
      "epoch 673/2000   error=501.631513\n",
      "epoch 674/2000   error=499.990889\n",
      "epoch 675/2000   error=498.358521\n",
      "epoch 676/2000   error=496.734390\n",
      "epoch 677/2000   error=495.118473\n",
      "epoch 678/2000   error=493.510751\n",
      "epoch 679/2000   error=491.911202\n",
      "epoch 680/2000   error=490.319803\n",
      "epoch 681/2000   error=488.736533\n",
      "epoch 682/2000   error=487.161369\n",
      "epoch 683/2000   error=485.594290\n",
      "epoch 684/2000   error=484.035273\n",
      "epoch 685/2000   error=482.484295\n",
      "epoch 686/2000   error=480.941334\n",
      "epoch 687/2000   error=479.406365\n",
      "epoch 688/2000   error=477.879366\n",
      "epoch 689/2000   error=476.360313\n",
      "epoch 690/2000   error=474.849182\n",
      "epoch 691/2000   error=473.345950\n",
      "epoch 692/2000   error=471.850591\n",
      "epoch 693/2000   error=470.363083\n",
      "epoch 694/2000   error=468.883399\n",
      "epoch 695/2000   error=467.411516\n",
      "epoch 696/2000   error=465.947409\n",
      "epoch 697/2000   error=464.491051\n",
      "epoch 698/2000   error=463.042419\n",
      "epoch 699/2000   error=461.601486\n",
      "epoch 700/2000   error=460.168227\n",
      "epoch 701/2000   error=458.742615\n",
      "epoch 702/2000   error=457.324626\n",
      "epoch 703/2000   error=455.914231\n",
      "epoch 704/2000   error=454.511406\n",
      "epoch 705/2000   error=453.116123\n",
      "epoch 706/2000   error=451.728356\n",
      "epoch 707/2000   error=450.348077\n",
      "epoch 708/2000   error=448.975261\n",
      "epoch 709/2000   error=447.609878\n",
      "epoch 710/2000   error=446.251903\n",
      "epoch 711/2000   error=444.901308\n",
      "epoch 712/2000   error=443.558064\n",
      "epoch 713/2000   error=442.222145\n",
      "epoch 714/2000   error=440.893521\n",
      "epoch 715/2000   error=439.572166\n",
      "epoch 716/2000   error=438.258051\n",
      "epoch 717/2000   error=436.951148\n",
      "epoch 718/2000   error=435.651428\n",
      "epoch 719/2000   error=434.358863\n",
      "epoch 720/2000   error=433.073424\n",
      "epoch 721/2000   error=431.795082\n",
      "epoch 722/2000   error=430.523810\n",
      "epoch 723/2000   error=429.259577\n",
      "epoch 724/2000   error=428.002356\n",
      "epoch 725/2000   error=426.752116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 726/2000   error=425.508829\n",
      "epoch 727/2000   error=424.272465\n",
      "epoch 728/2000   error=423.042996\n",
      "epoch 729/2000   error=421.820392\n",
      "epoch 730/2000   error=420.604624\n",
      "epoch 731/2000   error=419.395662\n",
      "epoch 732/2000   error=418.193476\n",
      "epoch 733/2000   error=416.998038\n",
      "epoch 734/2000   error=415.809317\n",
      "epoch 735/2000   error=414.627284\n",
      "epoch 736/2000   error=413.451910\n",
      "epoch 737/2000   error=412.283163\n",
      "epoch 738/2000   error=411.121015\n",
      "epoch 739/2000   error=409.965436\n",
      "epoch 740/2000   error=408.816395\n",
      "epoch 741/2000   error=407.673863\n",
      "epoch 742/2000   error=406.537810\n",
      "epoch 743/2000   error=405.408205\n",
      "epoch 744/2000   error=404.285018\n",
      "epoch 745/2000   error=403.168219\n",
      "epoch 746/2000   error=402.057778\n",
      "epoch 747/2000   error=400.953664\n",
      "epoch 748/2000   error=399.855847\n",
      "epoch 749/2000   error=398.764296\n",
      "epoch 750/2000   error=397.678981\n",
      "epoch 751/2000   error=396.599870\n",
      "epoch 752/2000   error=395.526933\n",
      "epoch 753/2000   error=394.460138\n",
      "epoch 754/2000   error=393.399456\n",
      "epoch 755/2000   error=392.344854\n",
      "epoch 756/2000   error=391.296302\n",
      "epoch 757/2000   error=390.253768\n",
      "epoch 758/2000   error=389.217220\n",
      "epoch 759/2000   error=388.186627\n",
      "epoch 760/2000   error=387.161957\n",
      "epoch 761/2000   error=386.143179\n",
      "epoch 762/2000   error=385.130260\n",
      "epoch 763/2000   error=384.123169\n",
      "epoch 764/2000   error=383.121873\n",
      "epoch 765/2000   error=382.126340\n",
      "epoch 766/2000   error=381.136538\n",
      "epoch 767/2000   error=380.152434\n",
      "epoch 768/2000   error=379.173996\n",
      "epoch 769/2000   error=378.201191\n",
      "epoch 770/2000   error=377.233987\n",
      "epoch 771/2000   error=376.272350\n",
      "epoch 772/2000   error=375.316248\n",
      "epoch 773/2000   error=374.365648\n",
      "epoch 774/2000   error=373.420517\n",
      "epoch 775/2000   error=372.480822\n",
      "epoch 776/2000   error=371.546530\n",
      "epoch 777/2000   error=370.617608\n",
      "epoch 778/2000   error=369.694023\n",
      "epoch 779/2000   error=368.775742\n",
      "epoch 780/2000   error=367.862733\n",
      "epoch 781/2000   error=366.954961\n",
      "epoch 782/2000   error=366.052395\n",
      "epoch 783/2000   error=365.155000\n",
      "epoch 784/2000   error=364.262746\n",
      "epoch 785/2000   error=363.375598\n",
      "epoch 786/2000   error=362.493524\n",
      "epoch 787/2000   error=361.616492\n",
      "epoch 788/2000   error=360.744468\n",
      "epoch 789/2000   error=359.877422\n",
      "epoch 790/2000   error=359.015319\n",
      "epoch 791/2000   error=358.158130\n",
      "epoch 792/2000   error=357.305820\n",
      "epoch 793/2000   error=356.458360\n",
      "epoch 794/2000   error=355.615716\n",
      "epoch 795/2000   error=354.777858\n",
      "epoch 796/2000   error=353.944755\n",
      "epoch 797/2000   error=353.116374\n",
      "epoch 798/2000   error=352.292687\n",
      "epoch 799/2000   error=351.473661\n",
      "epoch 800/2000   error=350.659266\n",
      "epoch 801/2000   error=349.849472\n",
      "epoch 802/2000   error=349.044250\n",
      "epoch 803/2000   error=348.243569\n",
      "epoch 804/2000   error=347.447399\n",
      "epoch 805/2000   error=346.655713\n",
      "epoch 806/2000   error=345.868479\n",
      "epoch 807/2000   error=345.085671\n",
      "epoch 808/2000   error=344.307259\n",
      "epoch 809/2000   error=343.533214\n",
      "epoch 810/2000   error=342.763510\n",
      "epoch 811/2000   error=341.998118\n",
      "epoch 812/2000   error=341.237011\n",
      "epoch 813/2000   error=340.480161\n",
      "epoch 814/2000   error=339.727542\n",
      "epoch 815/2000   error=338.979128\n",
      "epoch 816/2000   error=338.234891\n",
      "epoch 817/2000   error=337.494806\n",
      "epoch 818/2000   error=336.758846\n",
      "epoch 819/2000   error=336.026987\n",
      "epoch 820/2000   error=335.299204\n",
      "epoch 821/2000   error=334.575470\n",
      "epoch 822/2000   error=333.855762\n",
      "epoch 823/2000   error=333.140055\n",
      "epoch 824/2000   error=332.428324\n",
      "epoch 825/2000   error=331.720546\n",
      "epoch 826/2000   error=331.016697\n",
      "epoch 827/2000   error=330.316754\n",
      "epoch 828/2000   error=329.620692\n",
      "epoch 829/2000   error=328.928490\n",
      "epoch 830/2000   error=328.240123\n",
      "epoch 831/2000   error=327.555570\n",
      "epoch 832/2000   error=326.874807\n",
      "epoch 833/2000   error=326.197813\n",
      "epoch 834/2000   error=325.524564\n",
      "epoch 835/2000   error=324.855040\n",
      "epoch 836/2000   error=324.189216\n",
      "epoch 837/2000   error=323.527073\n",
      "epoch 838/2000   error=322.868587\n",
      "epoch 839/2000   error=322.213737\n",
      "epoch 840/2000   error=321.562501\n",
      "epoch 841/2000   error=320.914857\n",
      "epoch 842/2000   error=320.270784\n",
      "epoch 843/2000   error=319.630259\n",
      "epoch 844/2000   error=318.993262\n",
      "epoch 845/2000   error=318.359769\n",
      "epoch 846/2000   error=317.729760\n",
      "epoch 847/2000   error=317.103212\n",
      "epoch 848/2000   error=316.480103\n",
      "epoch 849/2000   error=315.860412\n",
      "epoch 850/2000   error=315.244116\n",
      "epoch 851/2000   error=314.631193\n",
      "epoch 852/2000   error=314.021621\n",
      "epoch 853/2000   error=313.415377\n",
      "epoch 854/2000   error=312.812439\n",
      "epoch 855/2000   error=312.212786\n",
      "epoch 856/2000   error=311.616393\n",
      "epoch 857/2000   error=311.023238\n",
      "epoch 858/2000   error=310.433299\n",
      "epoch 859/2000   error=309.846553\n",
      "epoch 860/2000   error=309.262977\n",
      "epoch 861/2000   error=308.682548\n",
      "epoch 862/2000   error=308.105243\n",
      "epoch 863/2000   error=307.531040\n",
      "epoch 864/2000   error=306.959915\n",
      "epoch 865/2000   error=306.391845\n",
      "epoch 866/2000   error=305.826807\n",
      "epoch 867/2000   error=305.264779\n",
      "epoch 868/2000   error=304.705737\n",
      "epoch 869/2000   error=304.149659\n",
      "epoch 870/2000   error=303.596522\n",
      "epoch 871/2000   error=303.046303\n",
      "epoch 872/2000   error=302.498980\n",
      "epoch 873/2000   error=301.954529\n",
      "epoch 874/2000   error=301.412930\n",
      "epoch 875/2000   error=300.874159\n",
      "epoch 876/2000   error=300.338194\n",
      "epoch 877/2000   error=299.805014\n",
      "epoch 878/2000   error=299.274596\n",
      "epoch 879/2000   error=298.746920\n",
      "epoch 880/2000   error=298.221963\n",
      "epoch 881/2000   error=297.699705\n",
      "epoch 882/2000   error=297.180124\n",
      "epoch 883/2000   error=296.663200\n",
      "epoch 884/2000   error=296.148912\n",
      "epoch 885/2000   error=295.637241\n",
      "epoch 886/2000   error=295.128165\n",
      "epoch 887/2000   error=294.621665\n",
      "epoch 888/2000   error=294.117721\n",
      "epoch 889/2000   error=293.616314\n",
      "epoch 890/2000   error=293.117425\n",
      "epoch 891/2000   error=292.621035\n",
      "epoch 892/2000   error=292.127126\n",
      "epoch 893/2000   error=291.635678\n",
      "epoch 894/2000   error=291.146674\n",
      "epoch 895/2000   error=290.660096\n",
      "epoch 896/2000   error=290.175926\n",
      "epoch 897/2000   error=289.694147\n",
      "epoch 898/2000   error=289.214742\n",
      "epoch 899/2000   error=288.737694\n",
      "epoch 900/2000   error=288.262985\n",
      "epoch 901/2000   error=287.790600\n",
      "epoch 902/2000   error=287.320522\n",
      "epoch 903/2000   error=286.852735\n",
      "epoch 904/2000   error=286.387223\n",
      "epoch 905/2000   error=285.923971\n",
      "epoch 906/2000   error=285.462962\n",
      "epoch 907/2000   error=285.004183\n",
      "epoch 908/2000   error=284.547616\n",
      "epoch 909/2000   error=284.093249\n",
      "epoch 910/2000   error=283.641065\n",
      "epoch 911/2000   error=283.191050\n",
      "epoch 912/2000   error=282.743189\n",
      "epoch 913/2000   error=282.297469\n",
      "epoch 914/2000   error=281.853875\n",
      "epoch 915/2000   error=281.412392\n",
      "epoch 916/2000   error=280.973007\n",
      "epoch 917/2000   error=280.535705\n",
      "epoch 918/2000   error=280.100474\n",
      "epoch 919/2000   error=279.667298\n",
      "epoch 920/2000   error=279.236165\n",
      "epoch 921/2000   error=278.807061\n",
      "epoch 922/2000   error=278.379972\n",
      "epoch 923/2000   error=277.954885\n",
      "epoch 924/2000   error=277.531787\n",
      "epoch 925/2000   error=277.110664\n",
      "epoch 926/2000   error=276.691502\n",
      "epoch 927/2000   error=276.274289\n",
      "epoch 928/2000   error=275.859012\n",
      "epoch 929/2000   error=275.445656\n",
      "epoch 930/2000   error=275.034210\n",
      "epoch 931/2000   error=274.624659\n",
      "epoch 932/2000   error=274.216992\n",
      "epoch 933/2000   error=273.811194\n",
      "epoch 934/2000   error=273.407252\n",
      "epoch 935/2000   error=273.005155\n",
      "epoch 936/2000   error=272.604888\n",
      "epoch 937/2000   error=272.206438\n",
      "epoch 938/2000   error=271.809794\n",
      "epoch 939/2000   error=271.414941\n",
      "epoch 940/2000   error=271.021867\n",
      "epoch 941/2000   error=270.630560\n",
      "epoch 942/2000   error=270.241005\n",
      "epoch 943/2000   error=269.853191\n",
      "epoch 944/2000   error=269.467104\n",
      "epoch 945/2000   error=269.082732\n",
      "epoch 946/2000   error=268.700062\n",
      "epoch 947/2000   error=268.319081\n",
      "epoch 948/2000   error=267.939777\n",
      "epoch 949/2000   error=267.562137\n",
      "epoch 950/2000   error=267.186148\n",
      "epoch 951/2000   error=266.811798\n",
      "epoch 952/2000   error=266.439074\n",
      "epoch 953/2000   error=266.067964\n",
      "epoch 954/2000   error=265.698456\n",
      "epoch 955/2000   error=265.330537\n",
      "epoch 956/2000   error=264.964194\n",
      "epoch 957/2000   error=264.599416\n",
      "epoch 958/2000   error=264.236191\n",
      "epoch 959/2000   error=263.874505\n",
      "epoch 960/2000   error=263.514348\n",
      "epoch 961/2000   error=263.155707\n",
      "epoch 962/2000   error=262.798571\n",
      "epoch 963/2000   error=262.442927\n",
      "epoch 964/2000   error=262.088763\n",
      "epoch 965/2000   error=261.736069\n",
      "epoch 966/2000   error=261.384832\n",
      "epoch 967/2000   error=261.035040\n",
      "epoch 968/2000   error=260.686683\n",
      "epoch 969/2000   error=260.339750\n",
      "epoch 970/2000   error=259.994227\n",
      "epoch 971/2000   error=259.650106\n",
      "epoch 972/2000   error=259.307373\n",
      "epoch 973/2000   error=258.966019\n",
      "epoch 974/2000   error=258.626032\n",
      "epoch 975/2000   error=258.287402\n",
      "epoch 976/2000   error=257.950117\n",
      "epoch 977/2000   error=257.614168\n",
      "epoch 978/2000   error=257.279542\n",
      "epoch 979/2000   error=256.946231\n",
      "epoch 980/2000   error=256.614223\n",
      "epoch 981/2000   error=256.283508\n",
      "epoch 982/2000   error=255.954075\n",
      "epoch 983/2000   error=255.625916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 984/2000   error=255.299018\n",
      "epoch 985/2000   error=254.973374\n",
      "epoch 986/2000   error=254.648972\n",
      "epoch 987/2000   error=254.325803\n",
      "epoch 988/2000   error=254.003856\n",
      "epoch 989/2000   error=253.683124\n",
      "epoch 990/2000   error=253.363595\n",
      "epoch 991/2000   error=253.045260\n",
      "epoch 992/2000   error=252.728110\n",
      "epoch 993/2000   error=252.412136\n",
      "epoch 994/2000   error=252.097329\n",
      "epoch 995/2000   error=251.783678\n",
      "epoch 996/2000   error=251.471176\n",
      "epoch 997/2000   error=251.159813\n",
      "epoch 998/2000   error=250.849581\n",
      "epoch 999/2000   error=250.540470\n",
      "epoch 1000/2000   error=250.232471\n",
      "epoch 1001/2000   error=249.925577\n",
      "epoch 1002/2000   error=249.619778\n",
      "epoch 1003/2000   error=249.315067\n",
      "epoch 1004/2000   error=249.011433\n",
      "epoch 1005/2000   error=248.708871\n",
      "epoch 1006/2000   error=248.407370\n",
      "epoch 1007/2000   error=248.106922\n",
      "epoch 1008/2000   error=247.807521\n",
      "epoch 1009/2000   error=247.509157\n",
      "epoch 1010/2000   error=247.211822\n",
      "epoch 1011/2000   error=246.915509\n",
      "epoch 1012/2000   error=246.620210\n",
      "epoch 1013/2000   error=246.325917\n",
      "epoch 1014/2000   error=246.032622\n",
      "epoch 1015/2000   error=245.740318\n",
      "epoch 1016/2000   error=245.448997\n",
      "epoch 1017/2000   error=245.158652\n",
      "epoch 1018/2000   error=244.869274\n",
      "epoch 1019/2000   error=244.580858\n",
      "epoch 1020/2000   error=244.293395\n",
      "epoch 1021/2000   error=244.006878\n",
      "epoch 1022/2000   error=243.721300\n",
      "epoch 1023/2000   error=243.436654\n",
      "epoch 1024/2000   error=243.152932\n",
      "epoch 1025/2000   error=242.870129\n",
      "epoch 1026/2000   error=242.588236\n",
      "epoch 1027/2000   error=242.307248\n",
      "epoch 1028/2000   error=242.027156\n",
      "epoch 1029/2000   error=241.747955\n",
      "epoch 1030/2000   error=241.469638\n",
      "epoch 1031/2000   error=241.192197\n",
      "epoch 1032/2000   error=240.915627\n",
      "epoch 1033/2000   error=240.639921\n",
      "epoch 1034/2000   error=240.365072\n",
      "epoch 1035/2000   error=240.091075\n",
      "epoch 1036/2000   error=239.817921\n",
      "epoch 1037/2000   error=239.545606\n",
      "epoch 1038/2000   error=239.274123\n",
      "epoch 1039/2000   error=239.003465\n",
      "epoch 1040/2000   error=238.733627\n",
      "epoch 1041/2000   error=238.464602\n",
      "epoch 1042/2000   error=238.196385\n",
      "epoch 1043/2000   error=237.928968\n",
      "epoch 1044/2000   error=237.662347\n",
      "epoch 1045/2000   error=237.396515\n",
      "epoch 1046/2000   error=237.131467\n",
      "epoch 1047/2000   error=236.867196\n",
      "epoch 1048/2000   error=236.603697\n",
      "epoch 1049/2000   error=236.340964\n",
      "epoch 1050/2000   error=236.078991\n",
      "epoch 1051/2000   error=235.817772\n",
      "epoch 1052/2000   error=235.557303\n",
      "epoch 1053/2000   error=235.297577\n",
      "epoch 1054/2000   error=235.038589\n",
      "epoch 1055/2000   error=234.780333\n",
      "epoch 1056/2000   error=234.522804\n",
      "epoch 1057/2000   error=234.265996\n",
      "epoch 1058/2000   error=234.009904\n",
      "epoch 1059/2000   error=233.754524\n",
      "epoch 1060/2000   error=233.499848\n",
      "epoch 1061/2000   error=233.245873\n",
      "epoch 1062/2000   error=232.992592\n",
      "epoch 1063/2000   error=232.740001\n",
      "epoch 1064/2000   error=232.488095\n",
      "epoch 1065/2000   error=232.236868\n",
      "epoch 1066/2000   error=231.986316\n",
      "epoch 1067/2000   error=231.736432\n",
      "epoch 1068/2000   error=231.487214\n",
      "epoch 1069/2000   error=231.238654\n",
      "epoch 1070/2000   error=230.990749\n",
      "epoch 1071/2000   error=230.743493\n",
      "epoch 1072/2000   error=230.496882\n",
      "epoch 1073/2000   error=230.250910\n",
      "epoch 1074/2000   error=230.005574\n",
      "epoch 1075/2000   error=229.760868\n",
      "epoch 1076/2000   error=229.516787\n",
      "epoch 1077/2000   error=229.273327\n",
      "epoch 1078/2000   error=229.030482\n",
      "epoch 1079/2000   error=228.788250\n",
      "epoch 1080/2000   error=228.546624\n",
      "epoch 1081/2000   error=228.305600\n",
      "epoch 1082/2000   error=228.065174\n",
      "epoch 1083/2000   error=227.825340\n",
      "epoch 1084/2000   error=227.586096\n",
      "epoch 1085/2000   error=227.347435\n",
      "epoch 1086/2000   error=227.109354\n",
      "epoch 1087/2000   error=226.871849\n",
      "epoch 1088/2000   error=226.634914\n",
      "epoch 1089/2000   error=226.398546\n",
      "epoch 1090/2000   error=226.162740\n",
      "epoch 1091/2000   error=225.927492\n",
      "epoch 1092/2000   error=225.692797\n",
      "epoch 1093/2000   error=225.458652\n",
      "epoch 1094/2000   error=225.225052\n",
      "epoch 1095/2000   error=224.991992\n",
      "epoch 1096/2000   error=224.759470\n",
      "epoch 1097/2000   error=224.527480\n",
      "epoch 1098/2000   error=224.296019\n",
      "epoch 1099/2000   error=224.065081\n",
      "epoch 1100/2000   error=223.834665\n",
      "epoch 1101/2000   error=223.604764\n",
      "epoch 1102/2000   error=223.375376\n",
      "epoch 1103/2000   error=223.146496\n",
      "epoch 1104/2000   error=222.918120\n",
      "epoch 1105/2000   error=222.690244\n",
      "epoch 1106/2000   error=222.462865\n",
      "epoch 1107/2000   error=222.235978\n",
      "epoch 1108/2000   error=222.009580\n",
      "epoch 1109/2000   error=221.783666\n",
      "epoch 1110/2000   error=221.558233\n",
      "epoch 1111/2000   error=221.333277\n",
      "epoch 1112/2000   error=221.108795\n",
      "epoch 1113/2000   error=220.884782\n",
      "epoch 1114/2000   error=220.661235\n",
      "epoch 1115/2000   error=220.438149\n",
      "epoch 1116/2000   error=220.215522\n",
      "epoch 1117/2000   error=219.993350\n",
      "epoch 1118/2000   error=219.771628\n",
      "epoch 1119/2000   error=219.550354\n",
      "epoch 1120/2000   error=219.329523\n",
      "epoch 1121/2000   error=219.109133\n",
      "epoch 1122/2000   error=218.889178\n",
      "epoch 1123/2000   error=218.669657\n",
      "epoch 1124/2000   error=218.450565\n",
      "epoch 1125/2000   error=218.231898\n",
      "epoch 1126/2000   error=218.013654\n",
      "epoch 1127/2000   error=217.795829\n",
      "epoch 1128/2000   error=217.578419\n",
      "epoch 1129/2000   error=217.361421\n",
      "epoch 1130/2000   error=217.144831\n",
      "epoch 1131/2000   error=216.928646\n",
      "epoch 1132/2000   error=216.712863\n",
      "epoch 1133/2000   error=216.497478\n",
      "epoch 1134/2000   error=216.282488\n",
      "epoch 1135/2000   error=216.067889\n",
      "epoch 1136/2000   error=215.853678\n",
      "epoch 1137/2000   error=215.639852\n",
      "epoch 1138/2000   error=215.426408\n",
      "epoch 1139/2000   error=215.213342\n",
      "epoch 1140/2000   error=215.000651\n",
      "epoch 1141/2000   error=214.788331\n",
      "epoch 1142/2000   error=214.576380\n",
      "epoch 1143/2000   error=214.364794\n",
      "epoch 1144/2000   error=214.153571\n",
      "epoch 1145/2000   error=213.942706\n",
      "epoch 1146/2000   error=213.732196\n",
      "epoch 1147/2000   error=213.522040\n",
      "epoch 1148/2000   error=213.312232\n",
      "epoch 1149/2000   error=213.102771\n",
      "epoch 1150/2000   error=212.893653\n",
      "epoch 1151/2000   error=212.684875\n",
      "epoch 1152/2000   error=212.476434\n",
      "epoch 1153/2000   error=212.268326\n",
      "epoch 1154/2000   error=212.060550\n",
      "epoch 1155/2000   error=211.853101\n",
      "epoch 1156/2000   error=211.645976\n",
      "epoch 1157/2000   error=211.439174\n",
      "epoch 1158/2000   error=211.232690\n",
      "epoch 1159/2000   error=211.026521\n",
      "epoch 1160/2000   error=210.820666\n",
      "epoch 1161/2000   error=210.615120\n",
      "epoch 1162/2000   error=210.409880\n",
      "epoch 1163/2000   error=210.204945\n",
      "epoch 1164/2000   error=210.000310\n",
      "epoch 1165/2000   error=209.795973\n",
      "epoch 1166/2000   error=209.591931\n",
      "epoch 1167/2000   error=209.388181\n",
      "epoch 1168/2000   error=209.184720\n",
      "epoch 1169/2000   error=208.981546\n",
      "epoch 1170/2000   error=208.778655\n",
      "epoch 1171/2000   error=208.576044\n",
      "epoch 1172/2000   error=208.373712\n",
      "epoch 1173/2000   error=208.171654\n",
      "epoch 1174/2000   error=207.969868\n",
      "epoch 1175/2000   error=207.768351\n",
      "epoch 1176/2000   error=207.567101\n",
      "epoch 1177/2000   error=207.366114\n",
      "epoch 1178/2000   error=207.165388\n",
      "epoch 1179/2000   error=206.964920\n",
      "epoch 1180/2000   error=206.764707\n",
      "epoch 1181/2000   error=206.564747\n",
      "epoch 1182/2000   error=206.365036\n",
      "epoch 1183/2000   error=206.165572\n",
      "epoch 1184/2000   error=205.966353\n",
      "epoch 1185/2000   error=205.767375\n",
      "epoch 1186/2000   error=205.568635\n",
      "epoch 1187/2000   error=205.370132\n",
      "epoch 1188/2000   error=205.171862\n",
      "epoch 1189/2000   error=204.973823\n",
      "epoch 1190/2000   error=204.776012\n",
      "epoch 1191/2000   error=204.578426\n",
      "epoch 1192/2000   error=204.381063\n",
      "epoch 1193/2000   error=204.183919\n",
      "epoch 1194/2000   error=203.986993\n",
      "epoch 1195/2000   error=203.790282\n",
      "epoch 1196/2000   error=203.593783\n",
      "epoch 1197/2000   error=203.397493\n",
      "epoch 1198/2000   error=203.201410\n",
      "epoch 1199/2000   error=203.005531\n",
      "epoch 1200/2000   error=202.809853\n",
      "epoch 1201/2000   error=202.614375\n",
      "epoch 1202/2000   error=202.419093\n",
      "epoch 1203/2000   error=202.224005\n",
      "epoch 1204/2000   error=202.029108\n",
      "epoch 1205/2000   error=201.834400\n",
      "epoch 1206/2000   error=201.639878\n",
      "epoch 1207/2000   error=201.445540\n",
      "epoch 1208/2000   error=201.251383\n",
      "epoch 1209/2000   error=201.057404\n",
      "epoch 1210/2000   error=200.863602\n",
      "epoch 1211/2000   error=200.669973\n",
      "epoch 1212/2000   error=200.476515\n",
      "epoch 1213/2000   error=200.283226\n",
      "epoch 1214/2000   error=200.090103\n",
      "epoch 1215/2000   error=199.897144\n",
      "epoch 1216/2000   error=199.704346\n",
      "epoch 1217/2000   error=199.511707\n",
      "epoch 1218/2000   error=199.319224\n",
      "epoch 1219/2000   error=199.126896\n",
      "epoch 1220/2000   error=198.934719\n",
      "epoch 1221/2000   error=198.742691\n",
      "epoch 1222/2000   error=198.550811\n",
      "epoch 1223/2000   error=198.359074\n",
      "epoch 1224/2000   error=198.167481\n",
      "epoch 1225/2000   error=197.976027\n",
      "epoch 1226/2000   error=197.784711\n",
      "epoch 1227/2000   error=197.593530\n",
      "epoch 1228/2000   error=197.402482\n",
      "epoch 1229/2000   error=197.211566\n",
      "epoch 1230/2000   error=197.020777\n",
      "epoch 1231/2000   error=196.830116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1232/2000   error=196.639578\n",
      "epoch 1233/2000   error=196.449163\n",
      "epoch 1234/2000   error=196.258868\n",
      "epoch 1235/2000   error=196.068691\n",
      "epoch 1236/2000   error=195.878629\n",
      "epoch 1237/2000   error=195.688681\n",
      "epoch 1238/2000   error=195.498845\n",
      "epoch 1239/2000   error=195.309119\n",
      "epoch 1240/2000   error=195.119500\n",
      "epoch 1241/2000   error=194.929987\n",
      "epoch 1242/2000   error=194.740578\n",
      "epoch 1243/2000   error=194.551271\n",
      "epoch 1244/2000   error=194.362063\n",
      "epoch 1245/2000   error=194.172955\n",
      "epoch 1246/2000   error=193.983942\n",
      "epoch 1247/2000   error=193.795024\n",
      "epoch 1248/2000   error=193.606200\n",
      "epoch 1249/2000   error=193.417466\n",
      "epoch 1250/2000   error=193.228822\n",
      "epoch 1251/2000   error=193.040267\n",
      "epoch 1252/2000   error=192.851797\n",
      "epoch 1253/2000   error=192.663413\n",
      "epoch 1254/2000   error=192.475112\n",
      "epoch 1255/2000   error=192.286893\n",
      "epoch 1256/2000   error=192.098755\n",
      "epoch 1257/2000   error=191.910696\n",
      "epoch 1258/2000   error=191.722716\n",
      "epoch 1259/2000   error=191.534812\n",
      "epoch 1260/2000   error=191.346983\n",
      "epoch 1261/2000   error=191.159229\n",
      "epoch 1262/2000   error=190.971549\n",
      "epoch 1263/2000   error=190.783940\n",
      "epoch 1264/2000   error=190.596403\n",
      "epoch 1265/2000   error=190.408937\n",
      "epoch 1266/2000   error=190.221540\n",
      "epoch 1267/2000   error=190.034212\n",
      "epoch 1268/2000   error=189.846951\n",
      "epoch 1269/2000   error=189.659758\n",
      "epoch 1270/2000   error=189.472632\n",
      "epoch 1271/2000   error=189.285571\n",
      "epoch 1272/2000   error=189.098576\n",
      "epoch 1273/2000   error=188.911646\n",
      "epoch 1274/2000   error=188.724781\n",
      "epoch 1275/2000   error=188.537979\n",
      "epoch 1276/2000   error=188.351242\n",
      "epoch 1277/2000   error=188.164569\n",
      "epoch 1278/2000   error=187.977959\n",
      "epoch 1279/2000   error=187.791413\n",
      "epoch 1280/2000   error=187.604930\n",
      "epoch 1281/2000   error=187.418510\n",
      "epoch 1282/2000   error=187.232154\n",
      "epoch 1283/2000   error=187.045861\n",
      "epoch 1284/2000   error=186.859633\n",
      "epoch 1285/2000   error=186.673468\n",
      "epoch 1286/2000   error=186.487368\n",
      "epoch 1287/2000   error=186.301332\n",
      "epoch 1288/2000   error=186.115361\n",
      "epoch 1289/2000   error=185.929456\n",
      "epoch 1290/2000   error=185.743617\n",
      "epoch 1291/2000   error=185.557845\n",
      "epoch 1292/2000   error=185.372139\n",
      "epoch 1293/2000   error=185.186502\n",
      "epoch 1294/2000   error=185.000932\n",
      "epoch 1295/2000   error=184.815432\n",
      "epoch 1296/2000   error=184.630002\n",
      "epoch 1297/2000   error=184.444643\n",
      "epoch 1298/2000   error=184.259355\n",
      "epoch 1299/2000   error=184.074140\n",
      "epoch 1300/2000   error=183.888997\n",
      "epoch 1301/2000   error=183.703930\n",
      "epoch 1302/2000   error=183.518937\n",
      "epoch 1303/2000   error=183.334020\n",
      "epoch 1304/2000   error=183.149180\n",
      "epoch 1305/2000   error=182.964418\n",
      "epoch 1306/2000   error=182.779735\n",
      "epoch 1307/2000   error=182.595133\n",
      "epoch 1308/2000   error=182.410611\n",
      "epoch 1309/2000   error=182.226171\n",
      "epoch 1310/2000   error=182.041815\n",
      "epoch 1311/2000   error=181.857542\n",
      "epoch 1312/2000   error=181.673355\n",
      "epoch 1313/2000   error=181.489253\n",
      "epoch 1314/2000   error=181.305239\n",
      "epoch 1315/2000   error=181.121313\n",
      "epoch 1316/2000   error=180.937475\n",
      "epoch 1317/2000   error=180.753728\n",
      "epoch 1318/2000   error=180.570071\n",
      "epoch 1319/2000   error=180.386507\n",
      "epoch 1320/2000   error=180.203035\n",
      "epoch 1321/2000   error=180.019656\n",
      "epoch 1322/2000   error=179.836372\n",
      "epoch 1323/2000   error=179.653182\n",
      "epoch 1324/2000   error=179.470089\n",
      "epoch 1325/2000   error=179.287092\n",
      "epoch 1326/2000   error=179.104192\n",
      "epoch 1327/2000   error=178.921390\n",
      "epoch 1328/2000   error=178.738687\n",
      "epoch 1329/2000   error=178.556082\n",
      "epoch 1330/2000   error=178.373577\n",
      "epoch 1331/2000   error=178.191172\n",
      "epoch 1332/2000   error=178.008868\n",
      "epoch 1333/2000   error=177.826664\n",
      "epoch 1334/2000   error=177.644562\n",
      "epoch 1335/2000   error=177.462560\n",
      "epoch 1336/2000   error=177.280661\n",
      "epoch 1337/2000   error=177.098863\n",
      "epoch 1338/2000   error=176.917168\n",
      "epoch 1339/2000   error=176.735574\n",
      "epoch 1340/2000   error=176.554083\n",
      "epoch 1341/2000   error=176.372694\n",
      "epoch 1342/2000   error=176.191407\n",
      "epoch 1343/2000   error=176.010222\n",
      "epoch 1344/2000   error=175.829140\n",
      "epoch 1345/2000   error=175.648159\n",
      "epoch 1346/2000   error=175.467280\n",
      "epoch 1347/2000   error=175.286502\n",
      "epoch 1348/2000   error=175.105826\n",
      "epoch 1349/2000   error=174.925250\n",
      "epoch 1350/2000   error=174.744775\n",
      "epoch 1351/2000   error=174.564400\n",
      "epoch 1352/2000   error=174.384125\n",
      "epoch 1353/2000   error=174.203949\n",
      "epoch 1354/2000   error=174.023872\n",
      "epoch 1355/2000   error=173.843894\n",
      "epoch 1356/2000   error=173.664013\n",
      "epoch 1357/2000   error=173.484230\n",
      "epoch 1358/2000   error=173.304543\n",
      "epoch 1359/2000   error=173.124953\n",
      "epoch 1360/2000   error=172.945458\n",
      "epoch 1361/2000   error=172.766058\n",
      "epoch 1362/2000   error=172.586752\n",
      "epoch 1363/2000   error=172.407540\n",
      "epoch 1364/2000   error=172.228421\n",
      "epoch 1365/2000   error=172.049395\n",
      "epoch 1366/2000   error=171.870460\n",
      "epoch 1367/2000   error=171.691615\n",
      "epoch 1368/2000   error=171.512862\n",
      "epoch 1369/2000   error=171.334197\n",
      "epoch 1370/2000   error=171.155622\n",
      "epoch 1371/2000   error=170.977135\n",
      "epoch 1372/2000   error=170.798735\n",
      "epoch 1373/2000   error=170.620422\n",
      "epoch 1374/2000   error=170.442195\n",
      "epoch 1375/2000   error=170.264054\n",
      "epoch 1376/2000   error=170.085998\n",
      "epoch 1377/2000   error=169.908026\n",
      "epoch 1378/2000   error=169.730137\n",
      "epoch 1379/2000   error=169.552331\n",
      "epoch 1380/2000   error=169.374608\n",
      "epoch 1381/2000   error=169.196966\n",
      "epoch 1382/2000   error=169.019405\n",
      "epoch 1383/2000   error=168.841925\n",
      "epoch 1384/2000   error=168.664524\n",
      "epoch 1385/2000   error=168.487204\n",
      "epoch 1386/2000   error=168.309962\n",
      "epoch 1387/2000   error=168.132798\n",
      "epoch 1388/2000   error=167.955713\n",
      "epoch 1389/2000   error=167.778705\n",
      "epoch 1390/2000   error=167.601774\n",
      "epoch 1391/2000   error=167.424920\n",
      "epoch 1392/2000   error=167.248142\n",
      "epoch 1393/2000   error=167.071440\n",
      "epoch 1394/2000   error=166.894813\n",
      "epoch 1395/2000   error=166.718262\n",
      "epoch 1396/2000   error=166.541786\n",
      "epoch 1397/2000   error=166.365385\n",
      "epoch 1398/2000   error=166.189058\n",
      "epoch 1399/2000   error=166.012806\n",
      "epoch 1400/2000   error=165.836628\n",
      "epoch 1401/2000   error=165.660523\n",
      "epoch 1402/2000   error=165.484492\n",
      "epoch 1403/2000   error=165.308535\n",
      "epoch 1404/2000   error=165.132652\n",
      "epoch 1405/2000   error=164.956842\n",
      "epoch 1406/2000   error=164.781106\n",
      "epoch 1407/2000   error=164.605442\n",
      "epoch 1408/2000   error=164.429853\n",
      "epoch 1409/2000   error=164.254337\n",
      "epoch 1410/2000   error=164.078894\n",
      "epoch 1411/2000   error=163.903524\n",
      "epoch 1412/2000   error=163.728229\n",
      "epoch 1413/2000   error=163.553007\n",
      "epoch 1414/2000   error=163.377858\n",
      "epoch 1415/2000   error=163.202784\n",
      "epoch 1416/2000   error=163.027784\n",
      "epoch 1417/2000   error=162.852858\n",
      "epoch 1418/2000   error=162.678006\n",
      "epoch 1419/2000   error=162.503229\n",
      "epoch 1420/2000   error=162.328527\n",
      "epoch 1421/2000   error=162.153900\n",
      "epoch 1422/2000   error=161.979348\n",
      "epoch 1423/2000   error=161.804872\n",
      "epoch 1424/2000   error=161.630471\n",
      "epoch 1425/2000   error=161.456146\n",
      "epoch 1426/2000   error=161.281898\n",
      "epoch 1427/2000   error=161.107726\n",
      "epoch 1428/2000   error=160.933632\n",
      "epoch 1429/2000   error=160.759614\n",
      "epoch 1430/2000   error=160.585674\n",
      "epoch 1431/2000   error=160.411811\n",
      "epoch 1432/2000   error=160.238027\n",
      "epoch 1433/2000   error=160.064321\n",
      "epoch 1434/2000   error=159.890694\n",
      "epoch 1435/2000   error=159.717145\n",
      "epoch 1436/2000   error=159.543676\n",
      "epoch 1437/2000   error=159.370287\n",
      "epoch 1438/2000   error=159.196978\n",
      "epoch 1439/2000   error=159.023749\n",
      "epoch 1440/2000   error=158.850600\n",
      "epoch 1441/2000   error=158.677533\n",
      "epoch 1442/2000   error=158.504546\n",
      "epoch 1443/2000   error=158.331641\n",
      "epoch 1444/2000   error=158.158818\n",
      "epoch 1445/2000   error=157.986077\n",
      "epoch 1446/2000   error=157.813418\n",
      "epoch 1447/2000   error=157.640842\n",
      "epoch 1448/2000   error=157.468349\n",
      "epoch 1449/2000   error=157.295939\n",
      "epoch 1450/2000   error=157.123612\n",
      "epoch 1451/2000   error=156.951369\n",
      "epoch 1452/2000   error=156.779209\n",
      "epoch 1453/2000   error=156.607134\n",
      "epoch 1454/2000   error=156.435143\n",
      "epoch 1455/2000   error=156.263236\n",
      "epoch 1456/2000   error=156.091414\n",
      "epoch 1457/2000   error=155.919677\n",
      "epoch 1458/2000   error=155.748025\n",
      "epoch 1459/2000   error=155.576458\n",
      "epoch 1460/2000   error=155.404976\n",
      "epoch 1461/2000   error=155.233580\n",
      "epoch 1462/2000   error=155.062269\n",
      "epoch 1463/2000   error=154.891043\n",
      "epoch 1464/2000   error=154.719904\n",
      "epoch 1465/2000   error=154.548850\n",
      "epoch 1466/2000   error=154.377882\n",
      "epoch 1467/2000   error=154.206999\n",
      "epoch 1468/2000   error=154.036203\n",
      "epoch 1469/2000   error=153.865493\n",
      "epoch 1470/2000   error=153.694868\n",
      "epoch 1471/2000   error=153.524330\n",
      "epoch 1472/2000   error=153.353877\n",
      "epoch 1473/2000   error=153.183511\n",
      "epoch 1474/2000   error=153.013230\n",
      "epoch 1475/2000   error=152.843035\n",
      "epoch 1476/2000   error=152.672926\n",
      "epoch 1477/2000   error=152.502903\n",
      "epoch 1478/2000   error=152.332965\n",
      "epoch 1479/2000   error=152.163113\n",
      "epoch 1480/2000   error=151.993346\n",
      "epoch 1481/2000   error=151.823664\n",
      "epoch 1482/2000   error=151.654068\n",
      "epoch 1483/2000   error=151.484557\n",
      "epoch 1484/2000   error=151.315131\n",
      "epoch 1485/2000   error=151.145789\n",
      "epoch 1486/2000   error=150.976533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1487/2000   error=150.807360\n",
      "epoch 1488/2000   error=150.638273\n",
      "epoch 1489/2000   error=150.469269\n",
      "epoch 1490/2000   error=150.300349\n",
      "epoch 1491/2000   error=150.131513\n",
      "epoch 1492/2000   error=149.962761\n",
      "epoch 1493/2000   error=149.794092\n",
      "epoch 1494/2000   error=149.625506\n",
      "epoch 1495/2000   error=149.457003\n",
      "epoch 1496/2000   error=149.288583\n",
      "epoch 1497/2000   error=149.120245\n",
      "epoch 1498/2000   error=148.951990\n",
      "epoch 1499/2000   error=148.783817\n",
      "epoch 1500/2000   error=148.615726\n",
      "epoch 1501/2000   error=148.447716\n",
      "epoch 1502/2000   error=148.279788\n",
      "epoch 1503/2000   error=148.111941\n",
      "epoch 1504/2000   error=147.944175\n",
      "epoch 1505/2000   error=147.776490\n",
      "epoch 1506/2000   error=147.608885\n",
      "epoch 1507/2000   error=147.441360\n",
      "epoch 1508/2000   error=147.273916\n",
      "epoch 1509/2000   error=147.106551\n",
      "epoch 1510/2000   error=146.939266\n",
      "epoch 1511/2000   error=146.772060\n",
      "epoch 1512/2000   error=146.604933\n",
      "epoch 1513/2000   error=146.437885\n",
      "epoch 1514/2000   error=146.270916\n",
      "epoch 1515/2000   error=146.104025\n",
      "epoch 1516/2000   error=145.937213\n",
      "epoch 1517/2000   error=145.770478\n",
      "epoch 1518/2000   error=145.603822\n",
      "epoch 1519/2000   error=145.437242\n",
      "epoch 1520/2000   error=145.270740\n",
      "epoch 1521/2000   error=145.104316\n",
      "epoch 1522/2000   error=144.937968\n",
      "epoch 1523/2000   error=144.771697\n",
      "epoch 1524/2000   error=144.605502\n",
      "epoch 1525/2000   error=144.439384\n",
      "epoch 1526/2000   error=144.273343\n",
      "epoch 1527/2000   error=144.107377\n",
      "epoch 1528/2000   error=143.941487\n",
      "epoch 1529/2000   error=143.775672\n",
      "epoch 1530/2000   error=143.609934\n",
      "epoch 1531/2000   error=143.444270\n",
      "epoch 1532/2000   error=143.278682\n",
      "epoch 1533/2000   error=143.113169\n",
      "epoch 1534/2000   error=142.947731\n",
      "epoch 1535/2000   error=142.782367\n",
      "epoch 1536/2000   error=142.617079\n",
      "epoch 1537/2000   error=142.451864\n",
      "epoch 1538/2000   error=142.286725\n",
      "epoch 1539/2000   error=142.121659\n",
      "epoch 1540/2000   error=141.956668\n",
      "epoch 1541/2000   error=141.791751\n",
      "epoch 1542/2000   error=141.626908\n",
      "epoch 1543/2000   error=141.462139\n",
      "epoch 1544/2000   error=141.297443\n",
      "epoch 1545/2000   error=141.132822\n",
      "epoch 1546/2000   error=140.968274\n",
      "epoch 1547/2000   error=140.803800\n",
      "epoch 1548/2000   error=140.639399\n",
      "epoch 1549/2000   error=140.475073\n",
      "epoch 1550/2000   error=140.310819\n",
      "epoch 1551/2000   error=140.146639\n",
      "epoch 1552/2000   error=139.982532\n",
      "epoch 1553/2000   error=139.818499\n",
      "epoch 1554/2000   error=139.654540\n",
      "epoch 1555/2000   error=139.490653\n",
      "epoch 1556/2000   error=139.326840\n",
      "epoch 1557/2000   error=139.163101\n",
      "epoch 1558/2000   error=138.999434\n",
      "epoch 1559/2000   error=138.835842\n",
      "epoch 1560/2000   error=138.672322\n",
      "epoch 1561/2000   error=138.508876\n",
      "epoch 1562/2000   error=138.345504\n",
      "epoch 1563/2000   error=138.182204\n",
      "epoch 1564/2000   error=138.018979\n",
      "epoch 1565/2000   error=137.855827\n",
      "epoch 1566/2000   error=137.692749\n",
      "epoch 1567/2000   error=137.529744\n",
      "epoch 1568/2000   error=137.366813\n",
      "epoch 1569/2000   error=137.203956\n",
      "epoch 1570/2000   error=137.041173\n",
      "epoch 1571/2000   error=136.878463\n",
      "epoch 1572/2000   error=136.715828\n",
      "epoch 1573/2000   error=136.553267\n",
      "epoch 1574/2000   error=136.390780\n",
      "epoch 1575/2000   error=136.228367\n",
      "epoch 1576/2000   error=136.066029\n",
      "epoch 1577/2000   error=135.903765\n",
      "epoch 1578/2000   error=135.741576\n",
      "epoch 1579/2000   error=135.579462\n",
      "epoch 1580/2000   error=135.417422\n",
      "epoch 1581/2000   error=135.255458\n",
      "epoch 1582/2000   error=135.093568\n",
      "epoch 1583/2000   error=134.931754\n",
      "epoch 1584/2000   error=134.770015\n",
      "epoch 1585/2000   error=134.608352\n",
      "epoch 1586/2000   error=134.446764\n",
      "epoch 1587/2000   error=134.285252\n",
      "epoch 1588/2000   error=134.123816\n",
      "epoch 1589/2000   error=133.962456\n",
      "epoch 1590/2000   error=133.801172\n",
      "epoch 1591/2000   error=133.639965\n",
      "epoch 1592/2000   error=133.478835\n",
      "epoch 1593/2000   error=133.317781\n",
      "epoch 1594/2000   error=133.156804\n",
      "epoch 1595/2000   error=132.995904\n",
      "epoch 1596/2000   error=132.835081\n",
      "epoch 1597/2000   error=132.674336\n",
      "epoch 1598/2000   error=132.513669\n",
      "epoch 1599/2000   error=132.353079\n",
      "epoch 1600/2000   error=132.192567\n",
      "epoch 1601/2000   error=132.032134\n",
      "epoch 1602/2000   error=131.871779\n",
      "epoch 1603/2000   error=131.711503\n",
      "epoch 1604/2000   error=131.551305\n",
      "epoch 1605/2000   error=131.391187\n",
      "epoch 1606/2000   error=131.231147\n",
      "epoch 1607/2000   error=131.071188\n",
      "epoch 1608/2000   error=130.911308\n",
      "epoch 1609/2000   error=130.751507\n",
      "epoch 1610/2000   error=130.591787\n",
      "epoch 1611/2000   error=130.432147\n",
      "epoch 1612/2000   error=130.272588\n",
      "epoch 1613/2000   error=130.113110\n",
      "epoch 1614/2000   error=129.953712\n",
      "epoch 1615/2000   error=129.794396\n",
      "epoch 1616/2000   error=129.635161\n",
      "epoch 1617/2000   error=129.476008\n",
      "epoch 1618/2000   error=129.316937\n",
      "epoch 1619/2000   error=129.157948\n",
      "epoch 1620/2000   error=128.999042\n",
      "epoch 1621/2000   error=128.840218\n",
      "epoch 1622/2000   error=128.681477\n",
      "epoch 1623/2000   error=128.522819\n",
      "epoch 1624/2000   error=128.364245\n",
      "epoch 1625/2000   error=128.205754\n",
      "epoch 1626/2000   error=128.047347\n",
      "epoch 1627/2000   error=127.889025\n",
      "epoch 1628/2000   error=127.730786\n",
      "epoch 1629/2000   error=127.572633\n",
      "epoch 1630/2000   error=127.414564\n",
      "epoch 1631/2000   error=127.256581\n",
      "epoch 1632/2000   error=127.098683\n",
      "epoch 1633/2000   error=126.940871\n",
      "epoch 1634/2000   error=126.783144\n",
      "epoch 1635/2000   error=126.625504\n",
      "epoch 1636/2000   error=126.467951\n",
      "epoch 1637/2000   error=126.310484\n",
      "epoch 1638/2000   error=126.153105\n",
      "epoch 1639/2000   error=125.995812\n",
      "epoch 1640/2000   error=125.838608\n",
      "epoch 1641/2000   error=125.681491\n",
      "epoch 1642/2000   error=125.524462\n",
      "epoch 1643/2000   error=125.367522\n",
      "epoch 1644/2000   error=125.210670\n",
      "epoch 1645/2000   error=125.053908\n",
      "epoch 1646/2000   error=124.897234\n",
      "epoch 1647/2000   error=124.740651\n",
      "epoch 1648/2000   error=124.584157\n",
      "epoch 1649/2000   error=124.427753\n",
      "epoch 1650/2000   error=124.271439\n",
      "epoch 1651/2000   error=124.115217\n",
      "epoch 1652/2000   error=123.959085\n",
      "epoch 1653/2000   error=123.803044\n",
      "epoch 1654/2000   error=123.647095\n",
      "epoch 1655/2000   error=123.491238\n",
      "epoch 1656/2000   error=123.335473\n",
      "epoch 1657/2000   error=123.179800\n",
      "epoch 1658/2000   error=123.024220\n",
      "epoch 1659/2000   error=122.868733\n",
      "epoch 1660/2000   error=122.713340\n",
      "epoch 1661/2000   error=122.558039\n",
      "epoch 1662/2000   error=122.402833\n",
      "epoch 1663/2000   error=122.247721\n",
      "epoch 1664/2000   error=122.092703\n",
      "epoch 1665/2000   error=121.937780\n",
      "epoch 1666/2000   error=121.782952\n",
      "epoch 1667/2000   error=121.628220\n",
      "epoch 1668/2000   error=121.473583\n",
      "epoch 1669/2000   error=121.319042\n",
      "epoch 1670/2000   error=121.164597\n",
      "epoch 1671/2000   error=121.010249\n",
      "epoch 1672/2000   error=120.855998\n",
      "epoch 1673/2000   error=120.701843\n",
      "epoch 1674/2000   error=120.547787\n",
      "epoch 1675/2000   error=120.393828\n",
      "epoch 1676/2000   error=120.239966\n",
      "epoch 1677/2000   error=120.086204\n",
      "epoch 1678/2000   error=119.932540\n",
      "epoch 1679/2000   error=119.778975\n",
      "epoch 1680/2000   error=119.625509\n",
      "epoch 1681/2000   error=119.472142\n",
      "epoch 1682/2000   error=119.318876\n",
      "epoch 1683/2000   error=119.165710\n",
      "epoch 1684/2000   error=119.012644\n",
      "epoch 1685/2000   error=118.859679\n",
      "epoch 1686/2000   error=118.706815\n",
      "epoch 1687/2000   error=118.554052\n",
      "epoch 1688/2000   error=118.401391\n",
      "epoch 1689/2000   error=118.248832\n",
      "epoch 1690/2000   error=118.096375\n",
      "epoch 1691/2000   error=117.944021\n",
      "epoch 1692/2000   error=117.791770\n",
      "epoch 1693/2000   error=117.639622\n",
      "epoch 1694/2000   error=117.487578\n",
      "epoch 1695/2000   error=117.335637\n",
      "epoch 1696/2000   error=117.183801\n",
      "epoch 1697/2000   error=117.032069\n",
      "epoch 1698/2000   error=116.880441\n",
      "epoch 1699/2000   error=116.728919\n",
      "epoch 1700/2000   error=116.577502\n",
      "epoch 1701/2000   error=116.426191\n",
      "epoch 1702/2000   error=116.274986\n",
      "epoch 1703/2000   error=116.123887\n",
      "epoch 1704/2000   error=115.972895\n",
      "epoch 1705/2000   error=115.822009\n",
      "epoch 1706/2000   error=115.671231\n",
      "epoch 1707/2000   error=115.520560\n",
      "epoch 1708/2000   error=115.369997\n",
      "epoch 1709/2000   error=115.219543\n",
      "epoch 1710/2000   error=115.069196\n",
      "epoch 1711/2000   error=114.918959\n",
      "epoch 1712/2000   error=114.768830\n",
      "epoch 1713/2000   error=114.618811\n",
      "epoch 1714/2000   error=114.468902\n",
      "epoch 1715/2000   error=114.319102\n",
      "epoch 1716/2000   error=114.169413\n",
      "epoch 1717/2000   error=114.019835\n",
      "epoch 1718/2000   error=113.870367\n",
      "epoch 1719/2000   error=113.721011\n",
      "epoch 1720/2000   error=113.571766\n",
      "epoch 1721/2000   error=113.422633\n",
      "epoch 1722/2000   error=113.273612\n",
      "epoch 1723/2000   error=113.124703\n",
      "epoch 1724/2000   error=112.975907\n",
      "epoch 1725/2000   error=112.827225\n",
      "epoch 1726/2000   error=112.678655\n",
      "epoch 1727/2000   error=112.530200\n",
      "epoch 1728/2000   error=112.381858\n",
      "epoch 1729/2000   error=112.233631\n",
      "epoch 1730/2000   error=112.085518\n",
      "epoch 1731/2000   error=111.937520\n",
      "epoch 1732/2000   error=111.789637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1733/2000   error=111.641870\n",
      "epoch 1734/2000   error=111.494219\n",
      "epoch 1735/2000   error=111.346683\n",
      "epoch 1736/2000   error=111.199264\n",
      "epoch 1737/2000   error=111.051962\n",
      "epoch 1738/2000   error=110.904777\n",
      "epoch 1739/2000   error=110.757709\n",
      "epoch 1740/2000   error=110.610759\n",
      "epoch 1741/2000   error=110.463927\n",
      "epoch 1742/2000   error=110.317213\n",
      "epoch 1743/2000   error=110.170618\n",
      "epoch 1744/2000   error=110.024141\n",
      "epoch 1745/2000   error=109.877783\n",
      "epoch 1746/2000   error=109.731545\n",
      "epoch 1747/2000   error=109.585427\n",
      "epoch 1748/2000   error=109.439429\n",
      "epoch 1749/2000   error=109.293551\n",
      "epoch 1750/2000   error=109.147794\n",
      "epoch 1751/2000   error=109.002157\n",
      "epoch 1752/2000   error=108.856642\n",
      "epoch 1753/2000   error=108.711249\n",
      "epoch 1754/2000   error=108.565977\n",
      "epoch 1755/2000   error=108.420827\n",
      "epoch 1756/2000   error=108.275800\n",
      "epoch 1757/2000   error=108.130896\n",
      "epoch 1758/2000   error=107.986114\n",
      "epoch 1759/2000   error=107.841456\n",
      "epoch 1760/2000   error=107.696922\n",
      "epoch 1761/2000   error=107.552511\n",
      "epoch 1762/2000   error=107.408225\n",
      "epoch 1763/2000   error=107.264063\n",
      "epoch 1764/2000   error=107.120026\n",
      "epoch 1765/2000   error=106.976114\n",
      "epoch 1766/2000   error=106.832327\n",
      "epoch 1767/2000   error=106.688666\n",
      "epoch 1768/2000   error=106.545131\n",
      "epoch 1769/2000   error=106.401722\n",
      "epoch 1770/2000   error=106.258440\n",
      "epoch 1771/2000   error=106.115284\n",
      "epoch 1772/2000   error=105.972256\n",
      "epoch 1773/2000   error=105.829355\n",
      "epoch 1774/2000   error=105.686582\n",
      "epoch 1775/2000   error=105.543936\n",
      "epoch 1776/2000   error=105.401419\n",
      "epoch 1777/2000   error=105.259031\n",
      "epoch 1778/2000   error=105.116771\n",
      "epoch 1779/2000   error=104.974640\n",
      "epoch 1780/2000   error=104.832639\n",
      "epoch 1781/2000   error=104.690767\n",
      "epoch 1782/2000   error=104.549025\n",
      "epoch 1783/2000   error=104.407414\n",
      "epoch 1784/2000   error=104.265933\n",
      "epoch 1785/2000   error=104.124583\n",
      "epoch 1786/2000   error=103.983364\n",
      "epoch 1787/2000   error=103.842276\n",
      "epoch 1788/2000   error=103.701320\n",
      "epoch 1789/2000   error=103.560496\n",
      "epoch 1790/2000   error=103.419804\n",
      "epoch 1791/2000   error=103.279244\n",
      "epoch 1792/2000   error=103.138817\n",
      "epoch 1793/2000   error=102.998524\n",
      "epoch 1794/2000   error=102.858363\n",
      "epoch 1795/2000   error=102.718336\n",
      "epoch 1796/2000   error=102.578443\n",
      "epoch 1797/2000   error=102.438683\n",
      "epoch 1798/2000   error=102.299059\n",
      "epoch 1799/2000   error=102.159569\n",
      "epoch 1800/2000   error=102.020213\n",
      "epoch 1801/2000   error=101.880993\n",
      "epoch 1802/2000   error=101.741908\n",
      "epoch 1803/2000   error=101.602959\n",
      "epoch 1804/2000   error=101.464146\n",
      "epoch 1805/2000   error=101.325470\n",
      "epoch 1806/2000   error=101.186929\n",
      "epoch 1807/2000   error=101.048526\n",
      "epoch 1808/2000   error=100.910259\n",
      "epoch 1809/2000   error=100.772130\n",
      "epoch 1810/2000   error=100.634138\n",
      "epoch 1811/2000   error=100.496284\n",
      "epoch 1812/2000   error=100.358568\n",
      "epoch 1813/2000   error=100.220990\n",
      "epoch 1814/2000   error=100.083551\n",
      "epoch 1815/2000   error=99.946251\n",
      "epoch 1816/2000   error=99.809089\n",
      "epoch 1817/2000   error=99.672067\n",
      "epoch 1818/2000   error=99.535185\n",
      "epoch 1819/2000   error=99.398443\n",
      "epoch 1820/2000   error=99.261840\n",
      "epoch 1821/2000   error=99.125378\n",
      "epoch 1822/2000   error=98.989056\n",
      "epoch 1823/2000   error=98.852875\n",
      "epoch 1824/2000   error=98.716836\n",
      "epoch 1825/2000   error=98.580937\n",
      "epoch 1826/2000   error=98.445180\n",
      "epoch 1827/2000   error=98.309565\n",
      "epoch 1828/2000   error=98.174092\n",
      "epoch 1829/2000   error=98.038761\n",
      "epoch 1830/2000   error=97.903573\n",
      "epoch 1831/2000   error=97.768528\n",
      "epoch 1832/2000   error=97.633625\n",
      "epoch 1833/2000   error=97.498866\n",
      "epoch 1834/2000   error=97.364250\n",
      "epoch 1835/2000   error=97.229778\n",
      "epoch 1836/2000   error=97.095450\n",
      "epoch 1837/2000   error=96.961266\n",
      "epoch 1838/2000   error=96.827226\n",
      "epoch 1839/2000   error=96.693331\n",
      "epoch 1840/2000   error=96.559581\n",
      "epoch 1841/2000   error=96.425976\n",
      "epoch 1842/2000   error=96.292517\n",
      "epoch 1843/2000   error=96.159203\n",
      "epoch 1844/2000   error=96.026034\n",
      "epoch 1845/2000   error=95.893012\n",
      "epoch 1846/2000   error=95.760136\n",
      "epoch 1847/2000   error=95.627406\n",
      "epoch 1848/2000   error=95.494823\n",
      "epoch 1849/2000   error=95.362387\n",
      "epoch 1850/2000   error=95.230098\n",
      "epoch 1851/2000   error=95.097956\n",
      "epoch 1852/2000   error=94.965962\n",
      "epoch 1853/2000   error=94.834115\n",
      "epoch 1854/2000   error=94.702417\n",
      "epoch 1855/2000   error=94.570867\n",
      "epoch 1856/2000   error=94.439465\n",
      "epoch 1857/2000   error=94.308212\n",
      "epoch 1858/2000   error=94.177107\n",
      "epoch 1859/2000   error=94.046152\n",
      "epoch 1860/2000   error=93.915345\n",
      "epoch 1861/2000   error=93.784689\n",
      "epoch 1862/2000   error=93.654182\n",
      "epoch 1863/2000   error=93.523824\n",
      "epoch 1864/2000   error=93.393617\n",
      "epoch 1865/2000   error=93.263560\n",
      "epoch 1866/2000   error=93.133653\n",
      "epoch 1867/2000   error=93.003897\n",
      "epoch 1868/2000   error=92.874292\n",
      "epoch 1869/2000   error=92.744838\n",
      "epoch 1870/2000   error=92.615536\n",
      "epoch 1871/2000   error=92.486384\n",
      "epoch 1872/2000   error=92.357385\n",
      "epoch 1873/2000   error=92.228537\n",
      "epoch 1874/2000   error=92.099841\n",
      "epoch 1875/2000   error=91.971297\n",
      "epoch 1876/2000   error=91.842906\n",
      "epoch 1877/2000   error=91.714667\n",
      "epoch 1878/2000   error=91.586581\n",
      "epoch 1879/2000   error=91.458648\n",
      "epoch 1880/2000   error=91.330868\n",
      "epoch 1881/2000   error=91.203241\n",
      "epoch 1882/2000   error=91.075768\n",
      "epoch 1883/2000   error=90.948449\n",
      "epoch 1884/2000   error=90.821283\n",
      "epoch 1885/2000   error=90.694271\n",
      "epoch 1886/2000   error=90.567414\n",
      "epoch 1887/2000   error=90.440711\n",
      "epoch 1888/2000   error=90.314162\n",
      "epoch 1889/2000   error=90.187768\n",
      "epoch 1890/2000   error=90.061529\n",
      "epoch 1891/2000   error=89.935446\n",
      "epoch 1892/2000   error=89.809517\n",
      "epoch 1893/2000   error=89.683744\n",
      "epoch 1894/2000   error=89.558126\n",
      "epoch 1895/2000   error=89.432664\n",
      "epoch 1896/2000   error=89.307358\n",
      "epoch 1897/2000   error=89.182208\n",
      "epoch 1898/2000   error=89.057214\n",
      "epoch 1899/2000   error=88.932376\n",
      "epoch 1900/2000   error=88.807695\n",
      "epoch 1901/2000   error=88.683171\n",
      "epoch 1902/2000   error=88.558803\n",
      "epoch 1903/2000   error=88.434593\n",
      "epoch 1904/2000   error=88.310539\n",
      "epoch 1905/2000   error=88.186643\n",
      "epoch 1906/2000   error=88.062904\n",
      "epoch 1907/2000   error=87.939323\n",
      "epoch 1908/2000   error=87.815900\n",
      "epoch 1909/2000   error=87.692634\n",
      "epoch 1910/2000   error=87.569526\n",
      "epoch 1911/2000   error=87.446577\n",
      "epoch 1912/2000   error=87.323786\n",
      "epoch 1913/2000   error=87.201153\n",
      "epoch 1914/2000   error=87.078679\n",
      "epoch 1915/2000   error=86.956364\n",
      "epoch 1916/2000   error=86.834207\n",
      "epoch 1917/2000   error=86.712210\n",
      "epoch 1918/2000   error=86.590371\n",
      "epoch 1919/2000   error=86.468692\n",
      "epoch 1920/2000   error=86.347172\n",
      "epoch 1921/2000   error=86.225812\n",
      "epoch 1922/2000   error=86.104611\n",
      "epoch 1923/2000   error=85.983570\n",
      "epoch 1924/2000   error=85.862689\n",
      "epoch 1925/2000   error=85.741968\n",
      "epoch 1926/2000   error=85.621407\n",
      "epoch 1927/2000   error=85.501006\n",
      "epoch 1928/2000   error=85.380766\n",
      "epoch 1929/2000   error=85.260686\n",
      "epoch 1930/2000   error=85.140767\n",
      "epoch 1931/2000   error=85.021008\n",
      "epoch 1932/2000   error=84.901410\n",
      "epoch 1933/2000   error=84.781974\n",
      "epoch 1934/2000   error=84.662698\n",
      "epoch 1935/2000   error=84.543583\n",
      "epoch 1936/2000   error=84.424630\n",
      "epoch 1937/2000   error=84.305838\n",
      "epoch 1938/2000   error=84.187208\n",
      "epoch 1939/2000   error=84.068739\n",
      "epoch 1940/2000   error=83.950432\n",
      "epoch 1941/2000   error=83.832287\n",
      "epoch 1942/2000   error=83.714303\n",
      "epoch 1943/2000   error=83.596482\n",
      "epoch 1944/2000   error=83.478822\n",
      "epoch 1945/2000   error=83.361325\n",
      "epoch 1946/2000   error=83.243990\n",
      "epoch 1947/2000   error=83.126818\n",
      "epoch 1948/2000   error=83.009807\n",
      "epoch 1949/2000   error=82.892960\n",
      "epoch 1950/2000   error=82.776275\n",
      "epoch 1951/2000   error=82.659753\n",
      "epoch 1952/2000   error=82.543393\n",
      "epoch 1953/2000   error=82.427197\n",
      "epoch 1954/2000   error=82.311163\n",
      "epoch 1955/2000   error=82.195293\n",
      "epoch 1956/2000   error=82.079586\n",
      "epoch 1957/2000   error=81.964042\n",
      "epoch 1958/2000   error=81.848661\n",
      "epoch 1959/2000   error=81.733444\n",
      "epoch 1960/2000   error=81.618390\n",
      "epoch 1961/2000   error=81.503499\n",
      "epoch 1962/2000   error=81.388772\n",
      "epoch 1963/2000   error=81.274209\n",
      "epoch 1964/2000   error=81.159810\n",
      "epoch 1965/2000   error=81.045574\n",
      "epoch 1966/2000   error=80.931503\n",
      "epoch 1967/2000   error=80.817595\n",
      "epoch 1968/2000   error=80.703851\n",
      "epoch 1969/2000   error=80.590271\n",
      "epoch 1970/2000   error=80.476856\n",
      "epoch 1971/2000   error=80.363604\n",
      "epoch 1972/2000   error=80.250517\n",
      "epoch 1973/2000   error=80.137594\n",
      "epoch 1974/2000   error=80.024836\n",
      "epoch 1975/2000   error=79.912241\n",
      "epoch 1976/2000   error=79.799812\n",
      "epoch 1977/2000   error=79.687547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1978/2000   error=79.575446\n",
      "epoch 1979/2000   error=79.463510\n",
      "epoch 1980/2000   error=79.351739\n",
      "epoch 1981/2000   error=79.240132\n",
      "epoch 1982/2000   error=79.128690\n",
      "epoch 1983/2000   error=79.017413\n",
      "epoch 1984/2000   error=78.906301\n",
      "epoch 1985/2000   error=78.795353\n",
      "epoch 1986/2000   error=78.684571\n",
      "epoch 1987/2000   error=78.573953\n",
      "epoch 1988/2000   error=78.463501\n",
      "epoch 1989/2000   error=78.353213\n",
      "epoch 1990/2000   error=78.243091\n",
      "epoch 1991/2000   error=78.133133\n",
      "epoch 1992/2000   error=78.023341\n",
      "epoch 1993/2000   error=77.913714\n",
      "epoch 1994/2000   error=77.804252\n",
      "epoch 1995/2000   error=77.694955\n",
      "epoch 1996/2000   error=77.585824\n",
      "epoch 1997/2000   error=77.476857\n",
      "epoch 1998/2000   error=77.368056\n",
      "epoch 1999/2000   error=77.259420\n",
      "epoch 2000/2000   error=77.150950\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=2000, learning_rate=0.0005, batch_size=10, regularisation=['L2', 0.00001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c663738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "err1 = net.errors[:1000]\n",
    "\n",
    "x = list(range(1, len(err1)+1))\n",
    "\n",
    "plt.plot(x, err1)\n",
    "\n",
    "plt.ylabel('Loss function')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7dca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713.51824963074\n"
     ]
    }
   ],
   "source": [
    "out = net.predict(x_test)\n",
    "mse = mse(out, y_test)\n",
    "print(mse) # 689.2154091460627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00274ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=10000, learning_rate=0.0005, batch_size=20, x_val=x_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "err2 = net.val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(err1)+1))\n",
    "\n",
    "plt.plot(x, err1, label='train')\n",
    "plt.plot(x, err2, label='val')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e579623",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.predict(x_test)\n",
    "mse = mse(out, y_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684771b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 100))\n",
    "net.add(ActivationLayer(sigmoid, derivative_sigmoid))\n",
    "net.add(Layer(100, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=10000, learning_rate=0.0005, batch_size=20,\n",
    "        early_stopping=3, x_val=x_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f505a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "err1 = net.errors\n",
    "err2 = net.val_errors\n",
    "\n",
    "x = list(range(1, len(err1)+1))\n",
    "\n",
    "plt.plot(x, err1, label='train')\n",
    "plt.plot(x, err2, label='val')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39282001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b6b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "net = Network()\n",
    "net.add(Layer(1, 50))\n",
    "net.add(ActivationLayer(tanh, derivative_sigmoid))\n",
    "net.add(Layer(50, 1))\n",
    "net.add(ActivationLayer(linear, derivative_linear))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=200, learning_rate=0.0005, batch_size=10, regularisation=['L2', 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.predict(x_test)\n",
    "mse = mse(out, y_test)\n",
    "print(mse) # 689.2154091460627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d4d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
